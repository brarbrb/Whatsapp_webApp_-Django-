{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3c9bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Capability:\", torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d42599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3576f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "dtype = None          # let Unsloth auto-choose (fp16/bf16)\n",
    "load_in_4bit = True   # VERY important for VRAM\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name      = \"unsloth/tinyllama-bnb-4bit\",  # 1.1B, 4-bit quantized\n",
    "    max_seq_length  = max_seq_length,\n",
    "    dtype           = dtype,\n",
    "    load_in_4bit    = load_in_4bit,\n",
    ")\n",
    "\n",
    "# --- 3. Add LoRA on top (PEFT) ---\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r                  = 16,  # low rank to save VRAM\n",
    "    target_modules     = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha         = 16,\n",
    "    lora_dropout       = 0,\n",
    "    bias               = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",  # helps on small GPUs\n",
    "    random_state       = 3407,\n",
    "    use_rslora         = False,\n",
    "    loftq_config       = None,\n",
    ")\n",
    "\n",
    "# --- 4. Tiny toy dataset (Alpaca-style) ---\n",
    "train_examples = [\n",
    "    {\n",
    "        \"instruction\": \"Say hello to the user named Areg.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Hello Areg! Nice to meet you.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain in one sentence what Django is.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Django is a high-level Python web framework that helps you build secure, scalable web apps quickly.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain in one sentence what WhatsApp Web is.\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"WhatsApp Web is the browser-based interface for using your WhatsApp account on a computer.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(train_examples)\n",
    "\n",
    "# Simple Alpaca-style formatting: Instruction + (optional) input + output\n",
    "def format_example(example):\n",
    "    if example[\"input\"]:\n",
    "        prompt = (\n",
    "            f\"### Instruction:\\n{example['instruction']}\\n\\n\"\n",
    "            f\"### Input:\\n{example['input']}\\n\\n\"\n",
    "            f\"### Response:\\n{example['output']}\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"### Instruction:\\n{example['instruction']}\\n\\n\"\n",
    "            f\"### Response:\\n{example['output']}\"\n",
    "        )\n",
    "    example[\"text\"] = prompt\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(format_example)\n",
    "\n",
    "# --- 5. Trainer setup ---\n",
    "# On Windows Unsloth recommends dataset_num_proc=1 to avoid crashes.\n",
    "# We also keep everything tiny: batch size 1, ~30 steps.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                      = \"tinyllama-unsloth-demo\",\n",
    "    per_device_train_batch_size     = 1,\n",
    "    gradient_accumulation_steps     = 1,\n",
    "    learning_rate                   = 2e-4,\n",
    "    max_steps                       = 30,   # tiny run â€“ just to test\n",
    "    warmup_steps                    = 5,\n",
    "    logging_steps                   = 5,\n",
    "    save_strategy                   = \"no\",\n",
    "    fp16                            = True,   # if this errors, set to False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model              = model,\n",
    "    tokenizer          = tokenizer,\n",
    "    train_dataset      = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length     = max_seq_length,\n",
    "    packing            = True,   # packs short samples together\n",
    "    dataset_num_proc   = 1,      # important for Windows\n",
    "    args               = training_args,\n",
    ")\n",
    "\n",
    "# --- 6. Run a tiny training loop ---\n",
    "trainer.train()\n",
    "\n",
    "# --- 7. Switch to inference mode + quick test ---\n",
    "FastLanguageModel.for_inference(model)  # disables grad, applies speedups\n",
    "\n",
    "prompt = \"### Instruction:\\nSay hello to Areg in a friendly way.\\n\\n### Response:\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = 64,\n",
    "        do_sample      = True,\n",
    "        top_p          = 0.9,\n",
    "        temperature    = 0.7,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whatsapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
