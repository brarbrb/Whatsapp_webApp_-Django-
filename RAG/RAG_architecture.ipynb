{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e316ea1c-d90a-4e3b-8797-0ead3fba3ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dd27c087",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import cohere\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b393e1aa-d3cd-4a69-8d6e-b7a5ced31609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "293MIWyDhO-8"
   },
   "source": [
    "API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b30f88da-7dc8-40ab-9089-c65edf790671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "33e6f5b3"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Replace with your own PineCone API KEY\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\", \"\") \n",
    "\n",
    "# Replace with your own Cohere API KEY\n",
    "COHERE_API_KEY = os.environ.get(\"COHERE_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_path = '/Users/ransela/Desktop/data_science_degree/4th_year/spring/Data Analysis and Visualization Lab/project/Whatsapp_webApp_-Django-/RAG/RAG_data/KB_data.csv'\n",
    "whatsapp_chats = pd.read_csv(kb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg_id</th>\n",
       "      <th>conv_id</th>\n",
       "      <th>conv_turn</th>\n",
       "      <th>sender_user_id</th>\n",
       "      <th>receiver_user_id</th>\n",
       "      <th>sent_at</th>\n",
       "      <th>text</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37135e2a-f6c5-482c-825b-362e797caf42</td>\n",
       "      <td>chat:u_barbara_u_maayan</td>\n",
       "      <td>1</td>\n",
       "      <td>u_barbara</td>\n",
       "      <td>u_maayan</td>\n",
       "      <td>2025-01-13 12:49:06</td>\n",
       "      <td>Hi, sweetheart.\\nTell me, do you happen to hav...</td>\n",
       "      <td>I broke up with my girlfriend today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2db1ff10-fe22-4e4a-aae5-f23947177c26</td>\n",
       "      <td>chat:u_barbara_u_maayan</td>\n",
       "      <td>2</td>\n",
       "      <td>u_maayan</td>\n",
       "      <td>u_barbara</td>\n",
       "      <td>2025-01-13 21:25:20</td>\n",
       "      <td>I broke up with my girlfriend today</td>\n",
       "      <td>By any chance, could you replace me face-to-fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5cc8ce01-f038-46d4-aa11-2a63650e79ee</td>\n",
       "      <td>chat:u_barbara_u_maayan</td>\n",
       "      <td>3</td>\n",
       "      <td>u_barbara</td>\n",
       "      <td>u_maayan</td>\n",
       "      <td>2025-01-13 21:25:20</td>\n",
       "      <td>By any chance, could you replace me face-to-fa...</td>\n",
       "      <td>That's the excuse\\nUmmmmm\\nDoubt it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34523a17-26eb-4f26-b0b4-342645dfa5d0</td>\n",
       "      <td>chat:u_barbara_u_maayan</td>\n",
       "      <td>4</td>\n",
       "      <td>u_maayan</td>\n",
       "      <td>u_barbara</td>\n",
       "      <td>2025-01-13 21:25:23</td>\n",
       "      <td>That's the excuse\\nUmmmmm\\nDoubt it</td>\n",
       "      <td>Oh no, sorryyyyy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b49c88d0-bb97-4d37-8e3e-40fd90943854</td>\n",
       "      <td>chat:u_barbara_u_maayan</td>\n",
       "      <td>5</td>\n",
       "      <td>u_barbara</td>\n",
       "      <td>u_maayan</td>\n",
       "      <td>2025-01-13 21:25:31</td>\n",
       "      <td>Oh no, sorryyyyy</td>\n",
       "      <td>In Karmiel?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 msg_id                  conv_id  conv_turn  \\\n",
       "0  37135e2a-f6c5-482c-825b-362e797caf42  chat:u_barbara_u_maayan          1   \n",
       "1  2db1ff10-fe22-4e4a-aae5-f23947177c26  chat:u_barbara_u_maayan          2   \n",
       "2  5cc8ce01-f038-46d4-aa11-2a63650e79ee  chat:u_barbara_u_maayan          3   \n",
       "3  34523a17-26eb-4f26-b0b4-342645dfa5d0  chat:u_barbara_u_maayan          4   \n",
       "4  b49c88d0-bb97-4d37-8e3e-40fd90943854  chat:u_barbara_u_maayan          5   \n",
       "\n",
       "  sender_user_id receiver_user_id              sent_at  \\\n",
       "0      u_barbara         u_maayan  2025-01-13 12:49:06   \n",
       "1       u_maayan        u_barbara  2025-01-13 21:25:20   \n",
       "2      u_barbara         u_maayan  2025-01-13 21:25:20   \n",
       "3       u_maayan        u_barbara  2025-01-13 21:25:23   \n",
       "4      u_barbara         u_maayan  2025-01-13 21:25:31   \n",
       "\n",
       "                                                text  \\\n",
       "0  Hi, sweetheart.\\nTell me, do you happen to hav...   \n",
       "1                I broke up with my girlfriend today   \n",
       "2  By any chance, could you replace me face-to-fa...   \n",
       "3                That's the excuse\\nUmmmmm\\nDoubt it   \n",
       "4                                   Oh no, sorryyyyy   \n",
       "\n",
       "                                              answer  \n",
       "0                I broke up with my girlfriend today  \n",
       "1  By any chance, could you replace me face-to-fa...  \n",
       "2                That's the excuse\\nUmmmmm\\nDoubt it  \n",
       "3                                   Oh no, sorryyyyy  \n",
       "4                                        In Karmiel?  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatsapp_chats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "messages sent to our user_id (u_barbara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_sent = whatsapp_chats[whatsapp_chats['receiver_user_id'] == 'u_barbara'].sort_values(by='conv_turn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8478c5f9-46af-4398-b6c0-2b68aca4bade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9tFpFp_4g_aF"
   },
   "source": [
    "## **Preprocessing & Embedding the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132a66f5-1927-4e33-a0ea-c4b4563256c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "25de8986"
   },
   "outputs": [],
   "source": [
    "def load_and_embedd_dataset(\n",
    "        dataset: DataFrame,\n",
    "        model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2'),\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Return a dataset with  column of embedd text field using a sentence-transformer model\n",
    "    Args:\n",
    "        dataset_name: The name of the dataset to load\n",
    "        model: The model to use for embedding\n",
    "    Returns:\n",
    "        tuple: A Dataset containing the new column of the the embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Loading and embedding the dataset\")\n",
    "\n",
    "    # build the text we embed\n",
    "    dataset[\"doc_text\"] = (\n",
    "        dataset[\"sender_user_id\"] + \": \" + dataset[\"text\"].fillna(\"\") + \"\\n\" +\n",
    "        dataset[\"receiver_user_id\"] + \": \" + dataset[\"answer\"].fillna(\"\")\n",
    "    )\n",
    "\n",
    "       \n",
    "    \n",
    "\n",
    "    # compute embeddings as numpy array [n_rows, dim]\n",
    "    embeddings = model.encode(\n",
    "        dataset[\"doc_text\"].tolist(),\n",
    "        batch_size=32,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    dataset[\"embedding\"] = list(embeddings)\n",
    "\n",
    "    print(\"Done!\")\n",
    "    return dataset, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e0e7e9-5f9c-47a8-97cd-a04dfd7eab35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9d558650"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
    "model_emb = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and embedding the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9/9 [00:01<00:00,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg_id</th>\n",
       "      <th>conv_id</th>\n",
       "      <th>conv_turn</th>\n",
       "      <th>sender_user_id</th>\n",
       "      <th>receiver_user_id</th>\n",
       "      <th>sent_at</th>\n",
       "      <th>text</th>\n",
       "      <th>answer</th>\n",
       "      <th>doc_text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2db1ff10-fe22-4e4a-aae5-f23947177c26</td>\n",
       "      <td>chat:u_barbara_u_maayan</td>\n",
       "      <td>2</td>\n",
       "      <td>u_maayan</td>\n",
       "      <td>u_barbara</td>\n",
       "      <td>2025-01-13 21:25:20</td>\n",
       "      <td>I broke up with my girlfriend today</td>\n",
       "      <td>By any chance, could you replace me face-to-fa...</td>\n",
       "      <td>u_maayan: I broke up with my girlfriend today\\...</td>\n",
       "      <td>[-0.106244296, 0.017279046, 0.08007642, 0.0052...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34523a17-26eb-4f26-b0b4-342645dfa5d0</td>\n",
       "      <td>chat:u_barbara_u_maayan</td>\n",
       "      <td>4</td>\n",
       "      <td>u_maayan</td>\n",
       "      <td>u_barbara</td>\n",
       "      <td>2025-01-13 21:25:23</td>\n",
       "      <td>That's the excuse\\nUmmmmm\\nDoubt it</td>\n",
       "      <td>Oh no, sorryyyyy</td>\n",
       "      <td>u_maayan: That's the excuse\\nUmmmmm\\nDoubt it\\...</td>\n",
       "      <td>[-0.009083027, 0.020246388, -0.0037002887, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6502b89a-e345-4fea-9577-8fc4ecf72772</td>\n",
       "      <td>chat:u_barbara_u_maayan</td>\n",
       "      <td>6</td>\n",
       "      <td>u_maayan</td>\n",
       "      <td>u_barbara</td>\n",
       "      <td>2025-01-13 21:25:33</td>\n",
       "      <td>In Karmiel?</td>\n",
       "      <td>No</td>\n",
       "      <td>u_maayan: In Karmiel?\\nu_barbara: No</td>\n",
       "      <td>[-0.048872855, 0.029274564, -0.049886603, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6b4fb49c-c428-44da-ba68-3e05cf1591a0</td>\n",
       "      <td>chat:u_barbara_u_maayan</td>\n",
       "      <td>8</td>\n",
       "      <td>u_maayan</td>\n",
       "      <td>u_barbara</td>\n",
       "      <td>2025-01-13 21:25:36</td>\n",
       "      <td>Ask Itay</td>\n",
       "      <td>In Haifa\\nFor 10th grade (Yud)</td>\n",
       "      <td>u_maayan: Ask Itay\\nu_barbara: In Haifa\\nFor 1...</td>\n",
       "      <td>[0.020345852, 0.07910001, -0.04930794, -0.0369...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>498fea35-9088-45a1-8358-0215e41cc717</td>\n",
       "      <td>chat:u_barbara_u_maayan</td>\n",
       "      <td>10</td>\n",
       "      <td>u_maayan</td>\n",
       "      <td>u_barbara</td>\n",
       "      <td>2025-01-13 21:25:40</td>\n",
       "      <td>Ah in Haifa\\nPossible then\\nAlthough I don't know</td>\n",
       "      <td>Keep me updated</td>\n",
       "      <td>u_maayan: Ah in Haifa\\nPossible then\\nAlthough...</td>\n",
       "      <td>[0.010905846, 0.018512694, -0.046605963, -0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 msg_id                  conv_id  conv_turn  \\\n",
       "1  2db1ff10-fe22-4e4a-aae5-f23947177c26  chat:u_barbara_u_maayan          2   \n",
       "3  34523a17-26eb-4f26-b0b4-342645dfa5d0  chat:u_barbara_u_maayan          4   \n",
       "5  6502b89a-e345-4fea-9577-8fc4ecf72772  chat:u_barbara_u_maayan          6   \n",
       "7  6b4fb49c-c428-44da-ba68-3e05cf1591a0  chat:u_barbara_u_maayan          8   \n",
       "9  498fea35-9088-45a1-8358-0215e41cc717  chat:u_barbara_u_maayan         10   \n",
       "\n",
       "  sender_user_id receiver_user_id              sent_at  \\\n",
       "1       u_maayan        u_barbara  2025-01-13 21:25:20   \n",
       "3       u_maayan        u_barbara  2025-01-13 21:25:23   \n",
       "5       u_maayan        u_barbara  2025-01-13 21:25:33   \n",
       "7       u_maayan        u_barbara  2025-01-13 21:25:36   \n",
       "9       u_maayan        u_barbara  2025-01-13 21:25:40   \n",
       "\n",
       "                                                text  \\\n",
       "1                I broke up with my girlfriend today   \n",
       "3                That's the excuse\\nUmmmmm\\nDoubt it   \n",
       "5                                        In Karmiel?   \n",
       "7                                           Ask Itay   \n",
       "9  Ah in Haifa\\nPossible then\\nAlthough I don't know   \n",
       "\n",
       "                                              answer  \\\n",
       "1  By any chance, could you replace me face-to-fa...   \n",
       "3                                   Oh no, sorryyyyy   \n",
       "5                                                 No   \n",
       "7                     In Haifa\\nFor 10th grade (Yud)   \n",
       "9                                    Keep me updated   \n",
       "\n",
       "                                            doc_text  \\\n",
       "1  u_maayan: I broke up with my girlfriend today\\...   \n",
       "3  u_maayan: That's the excuse\\nUmmmmm\\nDoubt it\\...   \n",
       "5               u_maayan: In Karmiel?\\nu_barbara: No   \n",
       "7  u_maayan: Ask Itay\\nu_barbara: In Haifa\\nFor 1...   \n",
       "9  u_maayan: Ah in Haifa\\nPossible then\\nAlthough...   \n",
       "\n",
       "                                           embedding  \n",
       "1  [-0.106244296, 0.017279046, 0.08007642, 0.0052...  \n",
       "3  [-0.009083027, 0.020246388, -0.0037002887, -0....  \n",
       "5  [-0.048872855, 0.029274564, -0.049886603, 0.02...  \n",
       "7  [0.020345852, 0.07910001, -0.04930794, -0.0369...  \n",
       "9  [0.010905846, 0.018512694, -0.046605963, -0.02...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb_df,embeddings = load_and_embedd_dataset(messages_sent, model_emb)\n",
    "kb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c2be1a-bf87-4c80-96bf-39c1faee2217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (258, 384)\n"
     ]
    }
   ],
   "source": [
    "shape = embeddings.shape\n",
    "print(\"Embedding shape:\", shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Context and User_style examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a context window for desired conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(\n",
    "    df: pd.DataFrame,\n",
    "    conv_id: str,\n",
    "    k: int = 10,\n",
    "    text_col: str = \"text\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return a string with the last k turns from a given conversation.\n",
    "\n",
    "    Each line looks like:\n",
    "        <sender_user_id>: <message>\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with at least ['conv_id', 'sender_user_id', text_col, 'sent_at'].\n",
    "        conv_id: The conversation id to extract from.\n",
    "        k: Number of turns to include (from the end of the conversation).\n",
    "        text_col: Column name that holds the actual message text.\n",
    "\n",
    "    Returns:\n",
    "        A single multi line string that can be dropped straight into the prompt.\n",
    "    \"\"\"\n",
    "    conv_df = (\n",
    "        df[df[\"conv_id\"] == conv_id]\n",
    "        .sort_values(\"sent_at\")          # or 'conv_turn' if you prefer\n",
    "        .tail(k)\n",
    "    )\n",
    "\n",
    "    lines = [\n",
    "        f\"{row['sender_user_id']}: {row[text_col]}\"\n",
    "        for _, row in conv_df.iterrows()\n",
    "    ]\n",
    "\n",
    "    context = \"\\n\".join(lines)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a User's Style (sample of k mesages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_user_style(\n",
    "    df: pd.DataFrame,\n",
    "    user_id: str,\n",
    "    k: int = 10,\n",
    "    text_col: str = \"text\",\n",
    "    random_sample: bool = True,\n",
    "    seed: int | None = 42,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return a string that represents the typical style of a given user,\n",
    "    built from k of their messages.\n",
    "\n",
    "    Each line looks like:\n",
    "        <message>\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with at least ['sender_user_id', text_col].\n",
    "        user_id: The user whose style we want to capture.\n",
    "        k: Number of messages to use.\n",
    "        text_col: Column with the text of the message.\n",
    "        random_sample: If True sample k messages randomly, else take the last k.\n",
    "        seed: Random seed for reproducibility when random_sample is True.\n",
    "\n",
    "    Returns:\n",
    "        A single multi line string with example messages in the user's style.\n",
    "    \"\"\"\n",
    "    user_df = df[df[\"sender_user_id\"] == user_id].copy()\n",
    "\n",
    "    if len(user_df) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    user_df = user_df.sort_values(\"sent_at\")\n",
    "\n",
    "    if random_sample and len(user_df) > k:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.choice(user_df.index.to_list(), size=k, replace=False)\n",
    "        user_df = user_df.loc[idx].sort_values(\"sent_at\")\n",
    "    else:\n",
    "        user_df = user_df.tail(k)\n",
    "\n",
    "    lines = [str(msg) for msg in user_df[text_col].tolist()]\n",
    "    user_style = \"\\n\".join(lines)\n",
    "    return user_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d98ac3-8a78-4501-a86e-8a8a077d0bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "a8474a04"
   },
   "source": [
    "## **Inserting the data into Pinecone VectorDB**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d963b3ae-f2ed-4ceb-b86f-92913a351567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KD8nsD7kpP7V"
   },
   "source": [
    "### creating the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed35fb0c-41d5-4194-aedf-3e0cca32ed94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "de149555"
   },
   "outputs": [],
   "source": [
    "def create_pinecone_index(\n",
    "        index_name: str,\n",
    "        dimension: int,\n",
    "        metric: str = 'cosine',\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a pinecone index if it does not exist\n",
    "    Args:\n",
    "        index_name: The name of the index\n",
    "        dimension: The dimension of the index\n",
    "        metric: The metric to use for the index\n",
    "    Returns:\n",
    "        Pinecone: A pinecone object which can later be used for upserting vectors and connecting to VectorDBs\n",
    "    \"\"\"\n",
    "     \n",
    "    print(\"Creating a Pinecone index...\")\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "\n",
    "            metric=metric,\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "        )\n",
    "    print(\"Done!\")\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e59457-8e73-471b-aa5f-7e1aa2c21b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6e59ce9",
    "outputId": "932401f8-5671-48d4-d61d-2e5a0ac62147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a Pinecone index...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = 'chats-index'\n",
    "\n",
    "# Create the vector database\n",
    "# We are passing the index_name and the size of our embeddings\n",
    "pc = create_pinecone_index(INDEX_NAME, shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7190d0f1-a366-4dbe-a513-7bbbafaa85a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "99bdf2a1"
   },
   "source": [
    "inserting data into the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc8ecf5f-8efc-47a9-a86c-d772299dfa82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "f22e0176"
   },
   "outputs": [],
   "source": [
    "def upsert_vectors(\n",
    "        index: Pinecone,\n",
    "        dataset: DataFrame,\n",
    "        embeddings: np.ndarray,\n",
    "        batch_size: int = 128\n",
    "):\n",
    "    \"\"\"\n",
    "    Upsert vectors to a pinecone index\n",
    "    Args:\n",
    "        index: The pinecone index object\n",
    "        embeddings: The embeddings to upsert\n",
    "        dataset: The dataset containing the metadata\n",
    "        batch_size: The batch size to use for upserting\n",
    "    Returns:\n",
    "        An updated pinecone index\n",
    "    \"\"\"\n",
    "    print(\"Upserting the embeddings to the Pinecone index...\")\n",
    "    \n",
    "    # Get all column names except 'embedding' for metadata\n",
    "    metadata_fields = [col for col in dataset.columns if col != \"embedding\"]\n",
    "\n",
    "    # Generate unique IDs for each row\n",
    "    ids = [str(i) for i in range(shape[0])]\n",
    "\n",
    "    # Build metadata dict for each row\n",
    "    meta = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        entry = {col: row[col] for col in metadata_fields}\n",
    "        meta.append(entry)  # Extract full metadata\n",
    "\n",
    "    # Create list of (id, vector, metadata) tuples for upserting\n",
    "    to_upsert = list(zip(ids, embeddings, meta))\n",
    "\n",
    "    # Upsert in batches\n",
    "    for i in tqdm(range(0, len(to_upsert), batch_size)):\n",
    "        i_end = min(i + batch_size, len(to_upsert))\n",
    "        index.upsert(vectors=to_upsert[i:i_end])\n",
    "\n",
    "    print(\"Upserting complete!\")\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1186b2-8274-4368-b144-79811a3b7354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee8f4096",
    "outputId": "e25b8f61-50f5-46d2-f0b6-9aa7f1451032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting the embeddings to the Pinecone index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Upsert the embeddings to the Pinecone index\n",
    "index = pc.Index(INDEX_NAME)\n",
    "index_upserted = upsert_vectors(index,messages_sent,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce560b74-82f2-4371-ae31-d722d45cbd6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "085fe0ef",
    "outputId": "f02cc313-6dcb-40a4-8297-344247d77700"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'': {'vector_count': 516}},\n",
       " 'total_vector_count': 516,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c06f0d13-5e9e-406e-a852-2f61b2288d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## creating propmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f36ca52-c8ce-4f29-a9ee-d01de5f3dcaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "bb91e350"
   },
   "outputs": [],
   "source": [
    "def augment_prompt(\n",
    "        query: str,\n",
    "        user_style: str,\n",
    "        context: str,\n",
    "        model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "        index=None,\n",
    ") -> str:\n",
    "\n",
    "    results = [float(val) for val in list(model.encode(query))]\n",
    "\n",
    "    # get top 10 results from knowledge base\n",
    "    query_results = index.query(\n",
    "        vector=results,\n",
    "        top_k=5,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )['matches']\n",
    "    text_matches = [match['metadata']['doc_text'] for match in query_results]\n",
    "\n",
    "\n",
    "    # get the text from the results\n",
    "    answers = \"\\n\\n\".join(text_matches)\n",
    "    \n",
    "\n",
    "    # feed into an augmented prompt\n",
    "    improved_prompt = f\"\"\"\n",
    "    \n",
    "      You write WhatsApp replies *exactly as the user would*.\n",
    "\n",
    "      Your job: given a new incoming message, write the reply the user is most likely to send.\n",
    "\n",
    "      You are given:\n",
    "      1) **query** – the new incoming message you must answer.\n",
    "      2) **similar_past_answers** – real replies the user wrote in the past to similar messages.  \n",
    "         Use them for tone, vibe, typical phrasing, emojis, and attitude.\n",
    "      3) **user_style_examples** – random messages the user wrote in other chats.  \n",
    "         Use them to mimic writing style, vocabulary, length, energy level, and emoji habits.\n",
    "      4) **recent_context** – the recent messages in this same chat (both sides).  \n",
    "         Your reply must fit naturally after this context.\n",
    "\n",
    "      ### Rules:\n",
    "      - Write the reply **as the user**, in first person.\n",
    "      - Match the **language**, **tone**, and **emotion** of the conversation.\n",
    "      - Keep it natural for WhatsApp: short to medium length, can include emojis.\n",
    "      - If the query contains multiple questions – answer all.\n",
    "      - If necessary info is missing – ask a short clarifying question.\n",
    "      - **Never** mention examples, past messages, embeddings, or that you're an AI.\n",
    "      - **Only output the final WhatsApp-style reply. No explanations.**\n",
    "\n",
    "      ---\n",
    "\n",
    "      ### query:\n",
    "      {query}\n",
    "\n",
    "      ### similar_past_answers for simillar queries:\n",
    "      {answers}\n",
    "\n",
    "      ### user_style_examples:\n",
    "      {user_style}\n",
    "\n",
    "      ### recent_context:\n",
    "      {context}\n",
    "      \"\"\"\n",
    "\n",
    "\n",
    "    return improved_prompt, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our Fine Tuned TinyLLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_model(model_path: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).to(DEVICE)\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "def llama_generate(prompt, model, tokenizer, max_new_tokens=80):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "    generated_ids = outputs[0][len(input_ids):]\n",
    "\n",
    "    answer = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_MODEL_PATH = \"/Users/ransela/merged\"\n",
    "fine_tuned_model, fine_tuned_tokenizer = load_model(FT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate response using augmented prompt from Fine Tuned TinyLLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ef8d52-fa80-42fe-babb-a895ff1b546a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "R71e3se0_f2e"
   },
   "outputs": [],
   "source": [
    "def generate_augmented_answer(query, user_style, context, model_to_gen, tokenizer_to_gen, model=model_emb, index=index):\n",
    "  augmented_prompt, source_knowledge, query_results = augment_prompt(query, user_style, context,model=model_emb,index=index)\n",
    "  answer = llama_generate(augmented_prompt, model_to_gen, tokenizer_to_gen)\n",
    "  return answer, source_knowledge, query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build context and user style strings\n",
    "context = build_context(kb_df, conv_id='chat:u_barbara_u_maayan', k=10)\n",
    "user_style = build_user_style(kb_df, user_id='u_barbara', k=10)\n",
    "\n",
    "\n",
    "query = \"i need help with my students, did you taught them already the embeddings ppt?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the full RAG + FT generation pipeline\n",
    "answer, retrieved_docs = generate_augmented_answer(\n",
    "    query=query,\n",
    "    user_style=user_style,\n",
    "    context=context,\n",
    "    model_to_gen=fine_tuned_model,\n",
    "    tokenizer_to_gen=fine_tuned_tokenizer,        \n",
    "    model=model_emb,          \n",
    "    index=index        # Pinecone index\n",
    ")\n",
    "\n",
    "# 4) Print results\n",
    "print(\"====== GENERATED ANSWER ======\")\n",
    "print(answer)\n",
    "\n",
    "# print(\"\\n====== RETRIEVED DOCUMENTS ======\")\n",
    "# for d in retrieved_docs:\n",
    "#     print(\"-\", d)\n",
    "\n",
    "\n",
    "for text_match in query_results:\n",
    "    print(\"----\" ,text_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmented_prompt, source_knowledge = augment_prompt(query, user_style, context,model=model_emb,index=index)\n",
    "co = cohere.Client(api_key=COHERE_API_KEY)\n",
    "response = co.chat(\n",
    "        model='command-a-03-2025',\n",
    "        message=augmented_prompt,\n",
    "    )\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG FOR LLAMA AND FINE_TUNED LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = (\n",
    "    \"\"\"\n",
    "    You are a friendly WhatsApp user named Barbara. Write a reply to the message: 'how are you?' \\n\n",
    "    Your response MUST be 1–2 short, casual sentences, starting immediately. \\n\n",
    "    Provide the reply text ONLY, without any instruction, context, quotes, or labels. \\n \n",
    "    \"\"\"\n",
    ")\n",
    "# The expected output should now be something clean like:\n",
    "# \"Hey! I'm good, just finishing up a lab project. How about you?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test generator with fine tuned llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you? I’m doing great. You? Same here. I hope you’re doing good too. We are. That sounds great. Thanks. You’re welcome. You'\n"
     ]
    }
   ],
   "source": [
    "print(llama_generate(test_prompt, fine_tuned_model, fine_tuned_tokenizer, max_new_tokens=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test generator with basic tiny llama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID).to(DEVICE)\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 2: \n",
      "    Message: 'How are you today, Barbara? Looking forward to catching up with you soon.' \n",
      "    Reply: 'Hey, Barbara\n"
     ]
    }
   ],
   "source": [
    "print(llama_generate(test_prompt, base_model, base_tokenizer, max_new_tokens=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RAG",
   "widgets": {}
  },
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
