{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed98d94",
   "metadata": {},
   "source": [
    "# Imports and env  settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37234b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"cohere\" \"datasets\" \"transformers\" \"accelerate\" \"peft\" \"bitsandbytes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7d4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, DataCollatorForLanguageModeling,Trainer,pipeline\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from typing import List, Dict, Optional, Any\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import cohere\n",
    "import json\n",
    "import os\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa51a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helpers & constants from the RAG file (generated automatically from ipynb)\n",
    "from RAG_generic_func import (\n",
    "    load_and_embedd_dataset,\n",
    "    create_pinecone_index,\n",
    "    upsert_vectors,       # we'll override here\n",
    "    build_context,\n",
    "    build_user_style,     # same\n",
    "    augment_prompt,\n",
    "    EMBEDDING_MODEL,\n",
    "    COHERE_API_KEY,\n",
    "    PINECONE_API_KEY,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ba9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COHERE_API_KEY = os.environ.get(\"COHERE_API_KEY_PAY\", \"\")\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\", \"\")\n",
    "\n",
    "INPUT_PATH_TRAIN = r\"C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\Fine_Tune\\fine_tune_data\\bbt_train_cleaned.jsonl\"\n",
    "INPUT_PATH_VAL   = r\"C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\Fine_Tune\\fine_tune_data\\bbt_val_cleaned.jsonl\"\n",
    "\n",
    "OUTPUT_PATH_TRAIN = r\"C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\Fine_Tune\\fine_tune_data\\bbt_train_distilled.jsonl\"\n",
    "OUTPUT_PATH_VAL   = r\"C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\Fine_Tune\\fine_tune_data\\bbt_val_distilled.jsonl\"\n",
    "\n",
    "MODEL_OUTPUT_PATH = r\"C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\Fine_Tune\\distilled\"\n",
    "data_files = {\n",
    "    \"train\": OUTPUT_PATH_TRAIN,\n",
    "    \"validation\": OUTPUT_PATH_VAL\n",
    "}\n",
    "# ds = load_dataset(\"json\", data_files=data_files)\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] # full attention\n",
    "# TARGET_MODULES = [\"q_proj\",\"v_proj\"]\n",
    "# TARGET_MODULES = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "\n",
    "\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "KB_PATH = r\"RAG_data\\KB_data.csv\"\n",
    "OUTPUT_KB_JSONL = r\"RAG_data\\distillation_dataset.jsonl\"\n",
    "\n",
    "AUGMENT_FRACTION = 0.3   # fraction of human examples that also get a teacher-label version\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "USER = \"Barbara\"         # change if your Barbara user_id is different\n",
    "INDEX_NAME = \"chats-index\"\n",
    "\n",
    "# parameters for fine tuning\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "BATCH_SIZE = 4\n",
    "MAX_LENGTH = 512\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# \"Weights\" via oversampling: how many times to duplicate human examples\n",
    "HUMAN_DUP_FACTOR = 2   # 2 = roughly double weight vs teacher\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ad3f2",
   "metadata": {},
   "source": [
    "# Distillation to improve fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7b2a0",
   "metadata": {},
   "source": [
    "## Preparing the distilled dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299648e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.ClientV2(COHERE_API_KEY)\n",
    "\n",
    "# Simple persona descriptions per speaker\n",
    "PERSONAS = {\n",
    "    \"Sheldon\": (\n",
    "        \"You are Sheldon Cooper from The Big Bang Theory. \"\n",
    "        \"You are a brilliant, pedantic theoretical physicist: literal, arrogant, and verbose. \"\n",
    "        \"You speak in precise, formal, slightly condescending language and often reference science, physics, and your own intellect.\"\n",
    "    ),\n",
    "    \"Leonard\": (\n",
    "        \"You are Leonard Hofstadter from The Big Bang Theory. \"\n",
    "        \"You are kind, self-conscious, often nervous, and try to keep the peace between your friends. \"\n",
    "        \"You speak in a casual, slightly awkward but caring tone, and you often try to sound reasonable and supportive.\"\n",
    "    ),\n",
    "    \"Penny\": (\n",
    "        \"You are Penny from The Big Bang Theory. \"\n",
    "        \"You are friendly, sarcastic, and down-to-earth, with good social intuition. \"\n",
    "        \"You use casual everyday language, sometimes tease the guys, and react emotionally and humorously to their geeky behavior.\"\n",
    "    ),\n",
    "    \"Howard\": (\n",
    "        \"You are Howard Wolowitz from The Big Bang Theory. \"\n",
    "        \"You are an aerospace engineer with an overconfident, sometimes creepy flirtatious style. \"\n",
    "        \"You crack innuendo-filled jokes, brag about your accomplishments, and speak in a playful, comedic tone, especially about space and women.\"\n",
    "    ),\n",
    "    \"Raj\": (\n",
    "        \"You are Rajesh Koothrappali from The Big Bang Theory. \"\n",
    "        \"You are sensitive, romantic, and somewhat socially awkward, with a love of pop culture and fantasy. \"\n",
    "        \"You speak in an emotional, sometimes dramatic way, and you often talk about love, loneliness, and your interests like movies and comics.\"\n",
    "    ),\n",
    "    \"Amy\": (\n",
    "        \"You are Amy Farrah Fowler from The Big Bang Theory. \"\n",
    "        \"You are a neurobiologist with a mix of scientific seriousness and socially awkward earnestness. \"\n",
    "        \"You speak in a formal, analytical tone about emotions and relationships, and you are intensely devoted to Sheldon and your friends.\"\n",
    "    ),\n",
    "    \"Bernadette\": (\n",
    "        \"You are Bernadette Rostenkowski-Wolowitz from The Big Bang Theory. \"\n",
    "        \"You have a sweet, high-pitched speaking style that can turn surprisingly strict or intimidating. \"\n",
    "        \"You are practical, sometimes bossy, and you often mix cute phrasing with sharp, no-nonsense comments.\"\n",
    "    ),\n",
    "    # other non-central charactes\n",
    "    \"DEFAULT\": (\n",
    "        \"You are a character from The Big Bang Theory. \"\n",
    "        \"Respond in a style consistent with that character's personality and the show's comedic tone.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def get_persona(speaker: str) -> str:\n",
    "    if not speaker:\n",
    "        return PERSONAS[\"DEFAULT\"]\n",
    "    return PERSONAS.get(speaker, PERSONAS[\"DEFAULT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distil_file(input_path: str, output_path: str, max_examples: int | None = None):\n",
    "    \"\"\"Reads original BBT JSONL and translates to teacher_target using Cohere\"\"\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        lines = [json.loads(l) for l in fin]\n",
    "\n",
    "    if max_examples is not None:\n",
    "        lines = lines[:max_examples]\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for ex in tqdm(lines, desc=f\"Distilling {input_path}\"):\n",
    "            prompt = ex.get(\"prompt\", \"\")\n",
    "            target_speaker = ex.get(\"target_speaker\", \"\")\n",
    "\n",
    "            persona = get_persona(target_speaker)\n",
    "\n",
    "            # We use Cohere chat endpoint with system + user message\n",
    "            try:\n",
    "                response = co.chat(\n",
    "                    model=\"command-a-03-2025\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": (\n",
    "                                persona\n",
    "                                + \" You will be given the dialogue context. \"\n",
    "                                  \"Continue the next line exactly as this character would speak. \"\n",
    "                                  \"Respond with ONLY the next line of dialogue, no quotes, \"\n",
    "                                  \"and do NOT add speaker tags.\"\n",
    "                            ),\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt,\n",
    "                        },\n",
    "                    ],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=96,\n",
    "                )\n",
    "                teacher_text = response.message.content[0].text.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error on example with ep={ex.get('ep')} scene={ex.get('scene')}:\", e)\n",
    "                # if Cohere fails we just use original script target \n",
    "                teacher_text = ex.get(\"target\", \"\").strip()\n",
    "\n",
    "            ex[\"teacher_target\"] = teacher_text\n",
    "\n",
    "            fout.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distil_file(INPUT_PATH_TRAIN, OUTPUT_PATH_TRAIN, max_examples=None) # meaning all - depends on restrictions of cohere account\n",
    "distil_file(INPUT_PATH_VAL, OUTPUT_PATH_VAL, max_examples=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6239ea9",
   "metadata": {},
   "source": [
    "## Running fine tune again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0014fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"Fine_Tune\\outputs\\tinyllama_bbt_distilled_lora\"\n",
    "\n",
    "ds = load_dataset(\"json\", data_files=data_files)\n",
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"validation\"]\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "\n",
    "base.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "model = get_peft_model(base, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe33f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "\n",
    "def build_example(ex):\n",
    "    prompt = ex.get(\"prompt\", \"\")\n",
    "    # Use teacher's answer as label (distillation)\n",
    "    target = ex.get(\"teacher_target\", ex.get(\"target\", \"\"))\n",
    "    x = prompt + target\n",
    "\n",
    "    enc_full   = tok(x, max_length=max_len, truncation=True)\n",
    "    enc_prompt = tok(prompt, max_length=max_len, truncation=True)\n",
    "\n",
    "    input_ids = enc_full[\"input_ids\"]\n",
    "    labels    = input_ids.copy()\n",
    "\n",
    "    # mask prompt part\n",
    "    n_prompt = len(enc_prompt[\"input_ids\"])\n",
    "    for i in range(min(n_prompt, len(labels))):\n",
    "        labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": enc_full[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "train_tok = train_ds.map(\n",
    "    build_example,\n",
    "    remove_columns=train_ds.column_names,\n",
    ")\n",
    "val_tok = val_ds.map(\n",
    "    build_example,\n",
    "    remove_columns=val_ds.column_names,\n",
    ")\n",
    "\n",
    "train_tok.set_format(type=\"torch\", columns=cols)\n",
    "val_tok.set_format(type=\"torch\", columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f069f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't want random masking (MLM); we already prepared labels ourselves.\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tok,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    fp16=True,           # if GPU supports it\n",
    "    bf16=False,          #  True if on A100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tok.save_pretrained(output_dir)\n",
    "print(\"Finished training + saved model + tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b9bb1b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e244f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30d5d4db",
   "metadata": {},
   "source": [
    "# Distillation on actual chats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb88098",
   "metadata": {},
   "source": [
    "## Getting teacher labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f7f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_style(\n",
    "    df: pd.DataFrame,\n",
    "    user_id: str,\n",
    "    k: int = 10,\n",
    "    text_col: str = \"text\",\n",
    "    random_sample: bool = True,\n",
    "    seed: int | None = 42,\n",
    ") -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      - list of example messages (lines)\n",
    "      - a single multi-line string user_style\n",
    "\n",
    "    If there are no messages for this user_id, returns ([], \"\").\n",
    "    \"\"\"\n",
    "    user_df = df[df[\"sender_user_id\"] == user_id].copy()\n",
    "\n",
    "    if len(user_df) == 0:\n",
    "        return [], \"\"\n",
    "\n",
    "    user_df = user_df.sort_values(\"sent_at\")\n",
    "\n",
    "    if random_sample and len(user_df) > k:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.choice(user_df.index.to_list(), size=k, replace=False)\n",
    "        user_df = user_df.loc[idx].sort_values(\"sent_at\")\n",
    "    else:\n",
    "        user_df = user_df.tail(k)\n",
    "\n",
    "    lines = [str(msg) for msg in user_df[text_col].tolist()]\n",
    "    user_style = \"\\n\".join(lines)\n",
    "    return lines, user_style\n",
    "\n",
    "\n",
    "def upsert_vectors(\n",
    "    index,               # Pinecone index object\n",
    "    dataset: pd.DataFrame,\n",
    "    embeddings: np.ndarray,\n",
    "    batch_size: int = 128,\n",
    "):\n",
    "    \"\"\"\n",
    "    Upsert vectors to a Pinecone index.\n",
    "\n",
    "    Args:\n",
    "        index: pc.Index instance.\n",
    "        dataset: DataFrame containing metadata; must align with embeddings.\n",
    "        embeddings: numpy array [n_rows, dim].\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    print(\"Upserting the embeddings to the Pinecone index...\")\n",
    "\n",
    "    if embeddings.shape[0] != len(dataset):\n",
    "        raise ValueError(\n",
    "            f\"Embeddings rows ({embeddings.shape[0]}) != dataset rows ({len(dataset)})\"\n",
    "        )\n",
    "\n",
    "    metadata_fields = [col for col in dataset.columns if col != \"embedding\"]\n",
    "\n",
    "    num_rows = embeddings.shape[0]\n",
    "    ids = [str(i) for i in range(num_rows)]\n",
    "\n",
    "    meta = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        entry = {col: row[col] for col in metadata_fields}\n",
    "        meta.append(entry)\n",
    "\n",
    "    to_upsert = list(zip(ids, embeddings, meta))\n",
    "\n",
    "    for i in tqdm(range(0, len(to_upsert), batch_size)):\n",
    "        i_end = min(i + batch_size, len(to_upsert))\n",
    "        index.upsert(vectors=to_upsert[i:i_end])\n",
    "\n",
    "    print(\"Upserting complete!\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ff9711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG initialization ===\n",
      "Loading and embedding the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a70a1ae9db4c2d9cd189055997af01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Rows for Barbara as receiver: 1181\n",
      "Creating a Pinecone index...\n",
      "Done!\n",
      "Upserting the embeddings to the Pinecone index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting complete!\n",
      "RAG initialization done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== RAG initialization ===\")\n",
    "\n",
    "# Cohere client\n",
    "co = cohere.Client(api_key=COHERE_API_KEY)\n",
    "\n",
    "# 1) Load KB\n",
    "whatsapp_chats = pd.read_csv(KB_PATH)\n",
    "\n",
    "# 2) Embed entire KB once\n",
    "model_emb = SentenceTransformer(EMBEDDING_MODEL)\n",
    "kb_df_all, embeddings = load_and_embedd_dataset(whatsapp_chats, model_emb)\n",
    "\n",
    "# 3) Keep only rows where Barbara is the receiver (for retrieval)\n",
    "kb_df_to_barbara = kb_df_all[kb_df_all[\"receiver_user_id\"] == USER].sort_values(\"conv_turn\")\n",
    "embeddings_to_barbara = embeddings[kb_df_to_barbara.index.to_list()]\n",
    "\n",
    "print(\"Rows for Barbara as receiver:\", len(kb_df_to_barbara))\n",
    "\n",
    "# 4) Create Pinecone index once\n",
    "pc = create_pinecone_index(INDEX_NAME, embeddings_to_barbara.shape[1])\n",
    "\n",
    "# 5) Upsert embeddings once\n",
    "index = pc.Index(INDEX_NAME)\n",
    "index = upsert_vectors(index, kb_df_to_barbara, embeddings_to_barbara)\n",
    "\n",
    "# 6) Shared context & style for Barbara (can tune conv_id)\n",
    "context = build_context(\n",
    "    kb_df_all,\n",
    "    conv_id=\"chat:u_1_u_2\",  # adjust conv_id as needed\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "_, user_style = build_user_style(\n",
    "    kb_df_all,\n",
    "    user_id=USER,\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "print(\"RAG initialization done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adeef431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohere_rag_answer(query: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Use cached embeddings + Pinecone index + user_style + context\n",
    "    to get a Cohere+RAG answer for a query.\n",
    "\n",
    "    Returns None if something fails.\n",
    "    \"\"\"\n",
    "    query = str(query).strip()\n",
    "    if not query:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        augmented_prompt, _ = augment_prompt(\n",
    "            query=query,\n",
    "            user_style=user_style,\n",
    "            context=context,\n",
    "            model=model_emb,\n",
    "            index=index,\n",
    "        )\n",
    "\n",
    "        response = co.chat(\n",
    "            model=\"command-a-03-2025\",\n",
    "            message=augmented_prompt,\n",
    "        )\n",
    "        text = response.text.strip()\n",
    "        if not text:\n",
    "            return None\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Cohere failed for query: {query[:60]!r}... ({e})\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_teacher_answer(query: str, kb_path: str = KB_PATH) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Wrapper used by the dataset builder.\n",
    "    Now uses the cached RAG state instead of re-embedding each time.\n",
    "    \"\"\"\n",
    "    return cohere_rag_answer(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b726ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_text(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Build the text that will go into the student model.\n",
    "    For now it's simple; later you can plug in full RAG context, etc.\n",
    "    \"\"\"\n",
    "    query = str(row[\"text\"]).strip()\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are {USER}. Answer in her natural WhatsApp style.\\n\\n\"\n",
    "        \"### QUERY\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"### INSTRUCTIONS\\n\"\n",
    "        f\"Reply as {USER} would reply in WhatsApp.\"\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e60e399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in KB: 2360\n",
      "Rows with receiver == 'Barbara' and non-empty human answer: 1181\n",
      "Base human examples: 1181\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(KB_PATH)\n",
    "\n",
    "df[\"answer\"] = df[\"answer\"].astype(str)\n",
    "mask_receiver_barbara = df[\"receiver_user_id\"] == USER\n",
    "\n",
    "# rows with non-empty human answer\n",
    "mask_has_human = df[\"answer\"].str.strip().ne(\"\")\n",
    "human_df = df[mask_receiver_barbara & mask_has_human].copy()\n",
    "print(f\"Total rows in KB: {len(df)}\")\n",
    "print(f\"Rows with receiver == {USER!r} and non-empty human answer: {len(human_df)}\")\n",
    "\n",
    "examples: List[Dict[str, Any]] = []\n",
    "for _, row in human_df.iterrows():\n",
    "    input_text = build_input_text(row)\n",
    "    human_answer = row[\"answer\"].strip()\n",
    "\n",
    "    examples.append({\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": human_answer,\n",
    "        \"label_source\": \"human\",   # used later for sampling/weighting\n",
    "    })\n",
    "\n",
    "print(f\"Base human examples: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3822fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will augment 354 rows with teacher answers\n",
      "Total examples after adding teacher labels: 1535\n"
     ]
    }
   ],
   "source": [
    "# randomly sampling rows where we add teacher\n",
    "indices = list(human_df.index)\n",
    "n_aug = int(AUGMENT_FRACTION * len(indices))\n",
    "augment_indices = set(random.sample(indices, n_aug))\n",
    "print(f\"Will augment {n_aug} rows with teacher answers\")\n",
    "\n",
    "for idx in augment_indices:\n",
    "    row = human_df.loc[idx]\n",
    "    query = str(row[\"text\"]).strip()\n",
    "    input_text = build_input_text(row)\n",
    "\n",
    "    teacher_answer = generate_teacher_answer(query)\n",
    "    if teacher_answer is None:\n",
    "        print(\"Haven't generated answer\")\n",
    "        continue\n",
    "\n",
    "    examples.append({\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": teacher_answer,\n",
    "        \"label_source\": \"teacher\",\n",
    "    })\n",
    "    \n",
    "print(f\"Total examples after adding teacher labels: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "040c3c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved distillation dataset to RAG_data\\distillation_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "out_df = pd.DataFrame(examples)\n",
    "out_df.to_json(\n",
    "    OUTPUT_KB_JSONL,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")\n",
    "print(f\"Saved distillation dataset to {OUTPUT_KB_JSONL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1925f7",
   "metadata": {},
   "source": [
    "## Cleaningthe dataset - Run only when the KB wasn't clean enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_from_input(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the text between '### QUERY' and '\\\\n\\\\n### INSTRUCTIONS'\n",
    "    from the input_text. Returns an empty string if pattern not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    marker_query = \"### QUERY\"\n",
    "    marker_instr = \"\\n\\n### INSTRUCTIONS\"\n",
    "    \n",
    "    pos_q = input_text.find(marker_query)\n",
    "    if pos_q == -1:\n",
    "        return \"\"\n",
    "    \n",
    "    # start after the line \"### QUERY\\n\"\n",
    "    pos_start = input_text.find(\"\\n\", pos_q)\n",
    "    if pos_start == -1:\n",
    "        return \"\"\n",
    "    pos_start += 1  # move past the newline\n",
    "    \n",
    "    pos_end = input_text.find(marker_instr, pos_start)\n",
    "    if pos_end == -1:\n",
    "        # take until the end if instructions marker not found\n",
    "        pos_end = len(input_text)\n",
    "    \n",
    "    query = input_text[pos_start:pos_end]\n",
    "    return query.strip()\n",
    "\n",
    "\n",
    "def cohere_barbara_reply(query: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Ask Cohere to answer as Barbara in WhatsApp style given just the query.\n",
    "    No RAG here - quick cleaning only.\n",
    "    \"\"\"\n",
    "    query = str(query).strip()\n",
    "    if not query:\n",
    "        return None\n",
    "    \n",
    "    prompt = (\n",
    "        \"You are Barbara. Answer in her natural WhatsApp style.\\n\\n\"\n",
    "        \"### QUERY\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"### INSTRUCTIONS\\n\"\n",
    "        \"Reply as Barbara would reply in WhatsApp. Use natural, short WhatsApp-style messages, \"\n",
    "        \"can include line breaks and emojis. Only output the reply, no explanations.\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        resp = co.chat(\n",
    "            model=\"command-r-08-2024\",\n",
    "            message=prompt,\n",
    "        )\n",
    "        text = resp.text.strip()\n",
    "        if not text:\n",
    "            return None\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Cohere failed for query: {query[:60]!r}... ({e})\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4196dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCRYPTION_LINE = (\n",
    "    \"Messages and calls are end-to-end encrypted. \"\n",
    "    \"Only people in this chat can read, listen to, or share them.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26d662f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(OUTPUT_KB_JSONL, lines=True)\n",
    "# Remove all mentions of the WhatsApp system message from input_text\n",
    "df[\"input_text\"] = df[\"input_text\"].str.replace(ENCRYPTION_LINE, \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d3957ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with audio-only query: 65\n",
      "Rows after dropping audio-only queries: 1470\n"
     ]
    }
   ],
   "source": [
    "def is_audio_only_query(input_text: str) -> bool:\n",
    "    q = extract_query_from_input(input_text)\n",
    "    # Clean possible invisible chars (like RTL mark) and lower\n",
    "    q_clean = q.replace(\"\\u200e\", \"\").strip().lower()\n",
    "    return (q_clean == \"audio omitted\") or (q_clean == \"\")\n",
    "\n",
    "\n",
    "audio_only_mask = df[\"input_text\"].apply(is_audio_only_query)\n",
    "print(\"Rows with audio-only query:\", audio_only_mask.sum())\n",
    "\n",
    "df = df[~audio_only_mask].copy()\n",
    "print(\"Rows after dropping audio-only queries:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cb0fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with 'audio omitted' in target_text: 153\n"
     ]
    }
   ],
   "source": [
    "# Find rows where the *answer* contains \"audio omitted\"\n",
    "def has_audio_omitted_answer(target_text: str) -> bool:\n",
    "    if not isinstance(target_text, str):\n",
    "        return False\n",
    "    t_clean = target_text.replace(\"\\u200e\", \"\").lower()\n",
    "    return \"audio omitted\" in t_clean\n",
    "\n",
    "mask_audio_answer = df[\"target_text\"].apply(has_audio_omitted_answer)\n",
    "print(\"Rows with 'audio omitted' in target_text:\", mask_audio_answer.sum())\n",
    "\n",
    "# For each such row: generate a new Barbara-style answer using Cohere\n",
    "rows_to_fix = df[mask_audio_answer].copy()\n",
    "\n",
    "for idx, row in rows_to_fix.iterrows():\n",
    "    query = extract_query_from_input(row[\"input_text\"])\n",
    "    new_answer = cohere_barbara_reply(query)\n",
    "    \n",
    "    if new_answer is not None:\n",
    "        df.at[idx, \"target_text\"] = new_answer\n",
    "    else:\n",
    "        # If Cohere fails for some reason, you can either:\n",
    "        #  - keep the old target_text, or\n",
    "        #  - drop the row. Let's drop to keep dataset clean.\n",
    "        df = df.drop(index=idx)\n",
    "        print(f\"Dropped row {idx} because Cohere couldn't generate answer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b6f4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_JSONL = r\"RAG_data\\distillation_dataset_clean.jsonl\"  # clean appended file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38b6ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned rows: 1470\n",
      "Appended cleaned rows to RAG_data\\distillation_dataset_clean.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"Final cleaned rows:\", len(df))\n",
    "\n",
    "# Append to the existing file instead of overwriting\n",
    "with open(OUTPUT_JSONL, \"a\", encoding=\"utf-8\") as f:\n",
    "    for _, row in df.iterrows():\n",
    "        f.write(json.dumps(row.to_dict(), ensure_ascii=False))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Appended cleaned rows to {OUTPUT_JSONL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071f93f",
   "metadata": {},
   "source": [
    "## Runing fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"Fine_Tune\\distilled\\KB_lora\"\n",
    "max_len = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207435cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_text', 'target_text', 'label_source'],\n",
      "    num_rows: 1470\n",
      "})\n",
      "Train size: 1323 | Val size: 147\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files={\"data\": OUTPUT_JSONL})[\"data\"]\n",
    "print(ds)\n",
    "\n",
    "splits = ds.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"| Val size:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0d8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train human: 1024 | Train teacher: 299\n",
      "New train size after oversampling: 2347\n"
     ]
    }
   ],
   "source": [
    "train_human   = train_ds.filter(lambda ex: ex.get(\"label_source\", \"\") == \"human\")\n",
    "train_teacher = train_ds.filter(lambda ex: ex.get(\"label_source\", \"\") == \"teacher\")\n",
    "print(\"Train human:\", len(train_human), \"| Train teacher:\", len(train_teacher))\n",
    "\n",
    "# oversanpling human responses by concatenating them HUMAN_DUP_FACTOR times\n",
    "train_human_oversampled = concatenate_datasets([train_human] * HUMAN_DUP_FACTOR)\n",
    "train_balanced = concatenate_datasets([train_human_oversampled, train_teacher]).shuffle(seed=42)\n",
    "train_ds = train_balanced\n",
    "print(\"New train size after oversampling:\", len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e761b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4436daec31ce42828a46a0f7bea9f2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65416e4644b54973a68243eee6db563d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc9af2d15d9468fa312beebdf3e5331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97dff55ff5ec4c5483120e3d6910b661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f1f4c4604e493aab93243cdb71d4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340387b6661f4bd88a161ae60697453e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tok.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      3\u001b[39m     tok.pad_token = tok.eos_token\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m base = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m base.config.pad_token_id = tok.pad_token_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4900\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4891\u001b[39m     gguf_file\n\u001b[32m   4892\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4893\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4894\u001b[39m ):\n\u001b[32m   4895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4896\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4897\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4898\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4900\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4902\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4907\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4913\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4920\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4921\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:1037\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1023\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m   1024\u001b[39m     cached_file_kwargs = {\n\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m   1026\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1035\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m: commit_hash,\n\u001b[32m   1036\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[32m   1040\u001b[39m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[32m   1041\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[32m   1042\u001b[39m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m         snapshot_download(\n\u001b[32m    495\u001b[39m             path_or_repo_id,\n\u001b[32m    496\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    506\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    989\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1168\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1181\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1735\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1728\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants.HF_HUB_DISABLE_XET:\n\u001b[32m   1729\u001b[39m             logger.warning(\n\u001b[32m   1730\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo, but the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhf_xet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1731\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFalling back to regular HTTP download. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1732\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1733\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1735\u001b[39m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1736\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1737\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1739\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1740\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1741\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1742\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1745\u001b[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:493\u001b[39m, in \u001b[36mhttp_get\u001b[39m\u001b[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[39m\n\u001b[32m    491\u001b[39m new_resume_size = resume_size\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[32m    495\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\urllib3\\response.py:1091\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1094\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\urllib3\\response.py:980\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    978\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\urllib3\\response.py:904\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    901\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    906\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    912\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\.venv\\Lib\\site-packages\\urllib3\\response.py:887\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    886\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "base.config.pad_token_id = tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "model = get_peft_model(base, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_example(ex: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Prompt from our dataset\n",
    "    input_text  = ex.get(\"input_text\", \"\")\n",
    "    target_text = ex.get(\"target_text\", \"\")\n",
    "\n",
    "    # Build full prompt + answer\n",
    "    prompt = input_text + \"\\n\\n### Barbara:\\n\"\n",
    "    x = prompt + target_text\n",
    "\n",
    "    # Tokenize full and prompt separately\n",
    "    enc_full   = tok(x, max_length=max_len, truncation=True)\n",
    "    enc_prompt = tok(prompt, max_length=max_len, truncation=True)\n",
    "\n",
    "    input_ids = enc_full[\"input_ids\"]\n",
    "    labels    = input_ids.copy()\n",
    "\n",
    "    # Mask prompt part in labels\n",
    "    n_prompt = len(enc_prompt[\"input_ids\"])\n",
    "    for i in range(min(n_prompt, len(labels))):\n",
    "        labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": enc_full[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146bf5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "train_tok = train_ds.map(\n",
    "    build_example,\n",
    "    remove_columns=train_ds.column_names,\n",
    ")\n",
    "val_tok = val_ds.map(\n",
    "    build_example,\n",
    "    remove_columns=val_ds.column_names,\n",
    ")\n",
    "\n",
    "train_tok.set_format(type=\"torch\", columns=cols)\n",
    "val_tok.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "len(train_tok), len(val_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eeb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tok,\n",
    "    mlm=False,   # no random masking; we already set labels\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    fp16=True,     # if GPU supports it\n",
    "    bf16=False,    # True if on A100 etc.\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)\n",
    "tok.save_pretrained(output_dir)\n",
    "print(\"Finished training + saved model + tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56de246",
   "metadata": {},
   "source": [
    "generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f8d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=output_dir,\n",
    "    tokenizer=tok,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "def generate_barbara_reply(query: str, max_new_tokens: int = 80):\n",
    "    input_text = (\n",
    "        \"You are Barbara. Answer in her natural WhatsApp style.\\n\\n\"\n",
    "        \"### QUERY\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"### INSTRUCTIONS\\n\"\n",
    "        \"Reply as Barbara would reply in WhatsApp.\"\n",
    "    )\n",
    "    prompt = input_text + \"\\n\\n### Barbara:\\n\"\n",
    "\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    if \"### Barbara:\" in out:\n",
    "        reply = out.split(\"### Barbara:\", 1)[1].strip()\n",
    "    else:\n",
    "        reply = out.strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "print(generate_barbara_reply(\"Hi, I'm sick today, can I get an extension?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
