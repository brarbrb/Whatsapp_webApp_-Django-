{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7a296d",
   "metadata": {},
   "source": [
    "In this part we applied a knowledge-distillation approach where a large model (Cohere Command + our RAG pipeline) acts as a teacher and a smaller TinyLlama model serves as the student.\n",
    "\n",
    "For selected messages, Cohere generated high-quality â€œteacher answersâ€ based on the query, context, retrieved similar messages, and examples of some user's style. The student model was then fine-tuned on a mixed dataset that included both the real human replies and these teacher-generated replies.\n",
    "\n",
    "To preserve users authentic style, human-labeled examples were oversampled, while teacher examples were added selectively.\n",
    "For some important inputs, we created two versions of the same training sampleâ€”one with the human answer and one with the teacher answerâ€”to enrich supervision.\n",
    "\n",
    "Through this setup, the student model learns user's tone while also benefiting from the teacher model clarity and completeness.\n",
    "\n",
    "Themain goal of this experimental part to see if the distillation technique can improve generalization and produces higher-quality from the smaller model. If so if we will have even bigger student model in theory we can dramatically reduce the resources in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed98d94",
   "metadata": {},
   "source": [
    "# Imports and env  settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37234b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"cohere\" \"datasets\" \"transformers\" \"accelerate\" \"peft\" \"bitsandbytes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6234de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ipywidgets dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from datasets import load_dataset, concatenate_datasets\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, DataCollatorForLanguageModeling,Trainer,pipeline\n",
    "# from peft import LoraConfig, get_peft_model, TaskType\n",
    "# from typing import List, Dict, Optional, Any\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "# from tqdm import tqdm\n",
    "# from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import random\n",
    "# import cohere\n",
    "# import json\n",
    "# import os\n",
    "# load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d41a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import helpers & constants from the RAG file (generated automatically from ipynb)\n",
    "# from RAG_generic_func import (\n",
    "#     load_and_embedd_dataset,\n",
    "#     create_pinecone_index,\n",
    "#     upsert_vectors,       # we'll override here\n",
    "#     build_context,\n",
    "#     build_user_style,     # same\n",
    "#     augment_prompt,\n",
    "#     EMBEDDING_MODEL,\n",
    "#     COHERE_API_KEY,\n",
    "#     PINECONE_API_KEY,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a4ff0e",
   "metadata": {},
   "source": [
    "When running on VM we got errors from imports that were used in distillation part so we created separate imports. Errors were mostly from newer versiom packages like numpy not supporting the GPU we used (it was older version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231ef118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, DataCollatorForLanguageModeling,Trainer, default_data_collator,BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from typing import List, Dict, Optional, Any\n",
    "import torch\n",
    "import numpy as np\n",
    "import os \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a9ba9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COHERE_API_KEY = os.environ.get(\"COHERE_API_KEY_PAY\", \"\")\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\", \"\")\n",
    "\n",
    "INPUT_PATH_TRAIN = r\"C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\Fine_Tune\\fine_tune_data\\bbt_train_cleaned.jsonl\"\n",
    "INPUT_PATH_VAL   = r\"C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\Fine_Tune\\fine_tune_data\\bbt_val_cleaned.jsonl\"\n",
    "\n",
    "OUTPUT_PATH_TRAIN = r\"C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\Fine_Tune\\fine_tune_data\\bbt_train_distilled.jsonl\"\n",
    "OUTPUT_PATH_VAL   = r\"C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\Fine_Tune\\fine_tune_data\\bbt_val_distilled.jsonl\"\n",
    "\n",
    "MODEL_OUTPUT_PATH = r\"C:\\Users\\Cyber_User\\Documents\\GitHub\\Whatsapp_webApp_-Django-\\Fine_Tune\\distilled\"\n",
    "data_files = {\n",
    "    \"train\": OUTPUT_PATH_TRAIN,\n",
    "    \"validation\": OUTPUT_PATH_VAL\n",
    "}\n",
    "# ds = load_dataset(\"json\", data_files=data_files)\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] # full attention\n",
    "# TARGET_MODULES = [\"q_proj\",\"v_proj\"]\n",
    "# TARGET_MODULES = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "\n",
    "\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "KB_PATH = r\"RAG_data\\KB_data.csv\"\n",
    "OUTPUT_KB_JSONL = r\"RAG_data\\distillation_dataset.jsonl\"\n",
    "\n",
    "AUGMENT_FRACTION = 0.3   # fraction of human examples that also get a teacher-label version\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "USER = \"Barbara\"         # change if your Barbara user_id is different\n",
    "INDEX_NAME = \"chats-index\"\n",
    "\n",
    "# parameters for fine tuning\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "BATCH_SIZE = 4\n",
    "MAX_LENGTH = 512\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# \"Weights\" via oversampling: how many times to duplicate human examples\n",
    "HUMAN_DUP_FACTOR = 2   # 2 = roughly double weight vs teacher\n",
    "OUTPUT_JSONL = \"RAG_data/distillation_dataset_clean.jsonl\"  # clean appended file\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5d4db",
   "metadata": {},
   "source": [
    "# Distillation to improve fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb88098",
   "metadata": {},
   "source": [
    "## Getting teacher labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f7f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_style(\n",
    "    df: pd.DataFrame,\n",
    "    user_id: str,\n",
    "    k: int = 10,\n",
    "    text_col: str = \"text\",\n",
    "    random_sample: bool = True,\n",
    "    seed: int | None = 42,\n",
    ") -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      - list of example messages (lines)\n",
    "      - a single multi-line string user_style\n",
    "\n",
    "    If there are no messages for this user_id, returns ([], \"\").\n",
    "    \"\"\"\n",
    "    user_df = df[df[\"sender_user_id\"] == user_id].copy()\n",
    "\n",
    "    if len(user_df) == 0:\n",
    "        return [], \"\"\n",
    "\n",
    "    user_df = user_df.sort_values(\"sent_at\")\n",
    "\n",
    "    if random_sample and len(user_df) > k:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.choice(user_df.index.to_list(), size=k, replace=False)\n",
    "        user_df = user_df.loc[idx].sort_values(\"sent_at\")\n",
    "    else:\n",
    "        user_df = user_df.tail(k)\n",
    "\n",
    "    lines = [str(msg) for msg in user_df[text_col].tolist()]\n",
    "    user_style = \"\\n\".join(lines)\n",
    "    return lines, user_style\n",
    "\n",
    "\n",
    "def upsert_vectors(\n",
    "    index,               # Pinecone index object\n",
    "    dataset: pd.DataFrame,\n",
    "    embeddings: np.ndarray,\n",
    "    batch_size: int = 128,\n",
    "):\n",
    "    \"\"\"\n",
    "    Upsert vectors to a Pinecone index.\n",
    "\n",
    "    Args:\n",
    "        index: pc.Index instance.\n",
    "        dataset: DataFrame containing metadata; must align with embeddings.\n",
    "        embeddings: numpy array [n_rows, dim].\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    print(\"Upserting the embeddings to the Pinecone index...\")\n",
    "\n",
    "    if embeddings.shape[0] != len(dataset):\n",
    "        raise ValueError(\n",
    "            f\"Embeddings rows ({embeddings.shape[0]}) != dataset rows ({len(dataset)})\"\n",
    "        )\n",
    "\n",
    "    metadata_fields = [col for col in dataset.columns if col != \"embedding\"]\n",
    "\n",
    "    num_rows = embeddings.shape[0]\n",
    "    ids = [str(i) for i in range(num_rows)]\n",
    "\n",
    "    meta = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        entry = {col: row[col] for col in metadata_fields}\n",
    "        meta.append(entry)\n",
    "\n",
    "    to_upsert = list(zip(ids, embeddings, meta))\n",
    "\n",
    "    for i in tqdm(range(0, len(to_upsert), batch_size)):\n",
    "        i_end = min(i + batch_size, len(to_upsert))\n",
    "        index.upsert(vectors=to_upsert[i:i_end])\n",
    "\n",
    "    print(\"Upserting complete!\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ff9711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG initialization ===\n",
      "Loading and embedding the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a70a1ae9db4c2d9cd189055997af01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Rows for Barbara as receiver: 1181\n",
      "Creating a Pinecone index...\n",
      "Done!\n",
      "Upserting the embeddings to the Pinecone index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:36<00:00,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting complete!\n",
      "RAG initialization done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== RAG initialization ===\")\n",
    "\n",
    "# Cohere client\n",
    "co = cohere.Client(api_key=COHERE_API_KEY)\n",
    "\n",
    "# 1) Load KB\n",
    "whatsapp_chats = pd.read_csv(KB_PATH)\n",
    "\n",
    "# 2) Embed entire KB once\n",
    "model_emb = SentenceTransformer(EMBEDDING_MODEL)\n",
    "kb_df_all, embeddings = load_and_embedd_dataset(whatsapp_chats, model_emb)\n",
    "\n",
    "# 3) Keep only rows where Barbara is the receiver (for retrieval)\n",
    "kb_df_to_barbara = kb_df_all[kb_df_all[\"receiver_user_id\"] == USER].sort_values(\"conv_turn\")\n",
    "embeddings_to_barbara = embeddings[kb_df_to_barbara.index.to_list()]\n",
    "\n",
    "print(\"Rows for Barbara as receiver:\", len(kb_df_to_barbara))\n",
    "\n",
    "# 4) Create Pinecone index once\n",
    "pc = create_pinecone_index(INDEX_NAME, embeddings_to_barbara.shape[1])\n",
    "\n",
    "# 5) Upsert embeddings once\n",
    "index = pc.Index(INDEX_NAME)\n",
    "index = upsert_vectors(index, kb_df_to_barbara, embeddings_to_barbara)\n",
    "\n",
    "# 6) Shared context & style for Barbara (can tune conv_id)\n",
    "context = build_context(\n",
    "    kb_df_all,\n",
    "    conv_id=\"chat:u_1_u_2\",  # adjust conv_id as needed\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "_, user_style = build_user_style(\n",
    "    kb_df_all,\n",
    "    user_id=USER,\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "print(\"RAG initialization done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adeef431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohere_rag_answer(query: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Use cached embeddings + Pinecone index + user_style + context\n",
    "    to get a Cohere+RAG answer for a query.\n",
    "\n",
    "    Returns None if something fails.\n",
    "    \"\"\"\n",
    "    query = str(query).strip()\n",
    "    if not query:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        augmented_prompt, _ = augment_prompt(\n",
    "            query=query,\n",
    "            user_style=user_style,\n",
    "            context=context,\n",
    "            model=model_emb,\n",
    "            index=index,\n",
    "        )\n",
    "\n",
    "        response = co.chat(\n",
    "            model=\"command-a-03-2025\",\n",
    "            message=augmented_prompt,\n",
    "        )\n",
    "        text = response.text.strip()\n",
    "        if not text:\n",
    "            return None\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Cohere failed for query: {query[:60]!r}... ({e})\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_teacher_answer(query: str, kb_path: str = KB_PATH) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Wrapper used by the dataset builder.\n",
    "    Now uses the cached RAG state instead of re-embedding each time.\n",
    "    \"\"\"\n",
    "    return cohere_rag_answer(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b726ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_text(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Build the text that will go into the student model.\n",
    "    For now it's simple; later you can plug in full RAG context, etc.\n",
    "    \"\"\"\n",
    "    query = str(row[\"text\"]).strip()\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are {USER}. Answer in her natural WhatsApp style.\\n\\n\"\n",
    "        \"### QUERY\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"### INSTRUCTIONS\\n\"\n",
    "        f\"Reply as {USER} would reply in WhatsApp.\"\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e60e399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in KB: 2360\n",
      "Rows with receiver == 'Barbara' and non-empty human answer: 1181\n",
      "Base human examples: 1181\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(KB_PATH)\n",
    "\n",
    "df[\"answer\"] = df[\"answer\"].astype(str)\n",
    "mask_receiver_barbara = df[\"receiver_user_id\"] == USER\n",
    "\n",
    "# rows with non-empty human answer\n",
    "mask_has_human = df[\"answer\"].str.strip().ne(\"\")\n",
    "human_df = df[mask_receiver_barbara & mask_has_human].copy()\n",
    "print(f\"Total rows in KB: {len(df)}\")\n",
    "print(f\"Rows with receiver == {USER!r} and non-empty human answer: {len(human_df)}\")\n",
    "\n",
    "examples: List[Dict[str, Any]] = []\n",
    "for _, row in human_df.iterrows():\n",
    "    input_text = build_input_text(row)\n",
    "    human_answer = row[\"answer\"].strip()\n",
    "\n",
    "    examples.append({\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": human_answer,\n",
    "        \"label_source\": \"human\",   # used later for sampling/weighting\n",
    "    })\n",
    "\n",
    "print(f\"Base human examples: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3822fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will augment 354 rows with teacher answers\n",
      "Total examples after adding teacher labels: 1535\n"
     ]
    }
   ],
   "source": [
    "# randomly sampling rows where we add teacher\n",
    "indices = list(human_df.index)\n",
    "n_aug = int(AUGMENT_FRACTION * len(indices))\n",
    "augment_indices = set(random.sample(indices, n_aug))\n",
    "print(f\"Will augment {n_aug} rows with teacher answers\")\n",
    "\n",
    "for idx in augment_indices:\n",
    "    row = human_df.loc[idx]\n",
    "    query = str(row[\"text\"]).strip()\n",
    "    input_text = build_input_text(row)\n",
    "\n",
    "    teacher_answer = generate_teacher_answer(query)\n",
    "    if teacher_answer is None:\n",
    "        print(\"Haven't generated answer\")\n",
    "        continue\n",
    "\n",
    "    examples.append({\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": teacher_answer,\n",
    "        \"label_source\": \"teacher\",\n",
    "    })\n",
    "    \n",
    "print(f\"Total examples after adding teacher labels: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "040c3c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved distillation dataset to RAG_data\\distillation_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "out_df = pd.DataFrame(examples)\n",
    "out_df.to_json(\n",
    "    OUTPUT_KB_JSONL,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")\n",
    "print(f\"Saved distillation dataset to {OUTPUT_KB_JSONL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1925f7",
   "metadata": {},
   "source": [
    "## Cleaningthe dataset - Run only when the KB wasn't clean enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_from_input(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the text between '### QUERY' and '\\\\n\\\\n### INSTRUCTIONS'\n",
    "    from the input_text. Returns an empty string if pattern not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    marker_query = \"### QUERY\"\n",
    "    marker_instr = \"\\n\\n### INSTRUCTIONS\"\n",
    "    \n",
    "    pos_q = input_text.find(marker_query)\n",
    "    if pos_q == -1:\n",
    "        return \"\"\n",
    "    \n",
    "    # start after the line \"### QUERY\\n\"\n",
    "    pos_start = input_text.find(\"\\n\", pos_q)\n",
    "    if pos_start == -1:\n",
    "        return \"\"\n",
    "    pos_start += 1  # move past the newline\n",
    "    \n",
    "    pos_end = input_text.find(marker_instr, pos_start)\n",
    "    if pos_end == -1:\n",
    "        # take until the end if instructions marker not found\n",
    "        pos_end = len(input_text)\n",
    "    \n",
    "    query = input_text[pos_start:pos_end]\n",
    "    return query.strip()\n",
    "\n",
    "\n",
    "def cohere_barbara_reply(query: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Ask Cohere to answer as Barbara in WhatsApp style given just the query.\n",
    "    No RAG here - quick cleaning only.\n",
    "    \"\"\"\n",
    "    query = str(query).strip()\n",
    "    if not query:\n",
    "        return None\n",
    "    \n",
    "    prompt = (\n",
    "        \"You are Barbara. Answer in her natural WhatsApp style.\\n\\n\"\n",
    "        \"### QUERY\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"### INSTRUCTIONS\\n\"\n",
    "        \"Reply as Barbara would reply in WhatsApp. Use natural, short WhatsApp-style messages, \"\n",
    "        \"can include line breaks and emojis. Only output the reply, no explanations.\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        resp = co.chat(\n",
    "            model=\"command-r-08-2024\",\n",
    "            message=prompt,\n",
    "        )\n",
    "        text = resp.text.strip()\n",
    "        if not text:\n",
    "            return None\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Cohere failed for query: {query[:60]!r}... ({e})\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4196dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCRYPTION_LINE = (\n",
    "    \"Messages and calls are end-to-end encrypted. \"\n",
    "    \"Only people in this chat can read, listen to, or share them.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26d662f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(OUTPUT_KB_JSONL, lines=True)\n",
    "# Remove all mentions of the WhatsApp system message from input_text\n",
    "df[\"input_text\"] = df[\"input_text\"].str.replace(ENCRYPTION_LINE, \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d3957ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with audio-only query: 65\n",
      "Rows after dropping audio-only queries: 1470\n"
     ]
    }
   ],
   "source": [
    "def is_audio_only_query(input_text: str) -> bool:\n",
    "    q = extract_query_from_input(input_text)\n",
    "    # Clean possible invisible chars (like RTL mark) and lower\n",
    "    q_clean = q.replace(\"\\u200e\", \"\").strip().lower()\n",
    "    return (q_clean == \"audio omitted\") or (q_clean == \"\")\n",
    "\n",
    "\n",
    "audio_only_mask = df[\"input_text\"].apply(is_audio_only_query)\n",
    "print(\"Rows with audio-only query:\", audio_only_mask.sum())\n",
    "\n",
    "df = df[~audio_only_mask].copy()\n",
    "print(\"Rows after dropping audio-only queries:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cb0fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with 'audio omitted' in target_text: 153\n"
     ]
    }
   ],
   "source": [
    "# Find rows where the *answer* contains \"audio omitted\"\n",
    "def has_audio_omitted_answer(target_text: str) -> bool:\n",
    "    if not isinstance(target_text, str):\n",
    "        return False\n",
    "    t_clean = target_text.replace(\"\\u200e\", \"\").lower()\n",
    "    return \"audio omitted\" in t_clean\n",
    "\n",
    "mask_audio_answer = df[\"target_text\"].apply(has_audio_omitted_answer)\n",
    "print(\"Rows with 'audio omitted' in target_text:\", mask_audio_answer.sum())\n",
    "\n",
    "# For each such row: generate a new Barbara-style answer using Cohere\n",
    "rows_to_fix = df[mask_audio_answer].copy()\n",
    "\n",
    "for idx, row in rows_to_fix.iterrows():\n",
    "    query = extract_query_from_input(row[\"input_text\"])\n",
    "    new_answer = cohere_barbara_reply(query)\n",
    "    \n",
    "    if new_answer is not None:\n",
    "        df.at[idx, \"target_text\"] = new_answer\n",
    "    else:\n",
    "        # If Cohere fails for some reason, you can either:\n",
    "        #  - keep the old target_text, or\n",
    "        #  - drop the row. Let's drop to keep dataset clean.\n",
    "        df = df.drop(index=idx)\n",
    "        print(f\"Dropped row {idx} because Cohere couldn't generate answer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b6f4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_JSONL = \"RAG_data/distillation_dataset_clean.jsonl\"  # clean appended file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38b6ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned rows: 1470\n",
      "Appended cleaned rows to RAG_data\\distillation_dataset_clean.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"Final cleaned rows:\", len(df))\n",
    "\n",
    "# Append to the existing file instead of overwriting\n",
    "with open(OUTPUT_JSONL, \"a\", encoding=\"utf-8\") as f:\n",
    "    for _, row in df.iterrows():\n",
    "        f.write(json.dumps(row.to_dict(), ensure_ascii=False))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Appended cleaned rows to {OUTPUT_JSONL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071f93f",
   "metadata": {},
   "source": [
    "## Runing fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a938bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/student/Whatsapp_webApp_-Django-/Fine_Tune/distilled/KB_lora\"\n",
    "max_len = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "207435cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_text', 'target_text', 'label_source'],\n",
      "    num_rows: 1470\n",
      "})\n",
      "Train size: 1323 | Val size: 147\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files={\"data\": OUTPUT_JSONL})[\"data\"]\n",
    "print(ds)\n",
    "\n",
    "splits = ds.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"| Val size:\", len(val_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e0d8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train human: 1024 | Train teacher: 299\n",
      "New train size after oversampling: 2347\n"
     ]
    }
   ],
   "source": [
    "train_human   = train_ds.filter(lambda ex: ex.get(\"label_source\", \"\") == \"human\")\n",
    "train_teacher = train_ds.filter(lambda ex: ex.get(\"label_source\", \"\") == \"teacher\")\n",
    "print(\"Train human:\", len(train_human), \"| Train teacher:\", len(train_teacher))\n",
    "\n",
    "# oversanpling human responses by concatenating them HUMAN_DUP_FACTOR times\n",
    "train_human_oversampled = concatenate_datasets([train_human] * HUMAN_DUP_FACTOR)\n",
    "train_balanced = concatenate_datasets([train_human_oversampled, train_teacher]).shuffle(seed=42)\n",
    "train_ds = train_balanced\n",
    "print(\"New train size after oversampling:\", len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91e761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "base.config.pad_token_id = tok.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d89798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "model = get_peft_model(base, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "# to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_example(ex: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # Prompt from our dataset\n",
    "    input_text  = ex.get(\"input_text\", \"\")\n",
    "    target_text = ex.get(\"target_text\", \"\")\n",
    "    prompt = input_text + \"\\n\\n### Barbara:\\n\"\n",
    "    x = prompt + target_text\n",
    "    # pad to fixed length here\n",
    "    enc_full   = tok(\n",
    "        x,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",      \n",
    "    )\n",
    "    enc_prompt = tok(\n",
    "        prompt,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",      # just so n_prompt is consistent\n",
    "    )\n",
    "\n",
    "    input_ids = enc_full[\"input_ids\"]\n",
    "    labels    = input_ids.copy()\n",
    "\n",
    "    # mask for prompt part in labels\n",
    "    n_prompt = len(enc_prompt[\"input_ids\"])\n",
    "    for i in range(min(n_prompt, len(labels))):\n",
    "        labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": enc_full[\"attention_mask\"],\n",
    "        \"labels\": labels, \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146bf5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2347, 147)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "train_tok = train_ds.map(\n",
    "    build_example,\n",
    "    remove_columns=train_ds.column_names,\n",
    ")\n",
    "val_tok = val_ds.map(\n",
    "    build_example,\n",
    "    remove_columns=val_ds.column_names,\n",
    ")\n",
    "\n",
    "train_tok.set_format(type=\"torch\", columns=cols)\n",
    "val_tok.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "len(train_tok), len(val_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d36fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc, torch\n",
    "\n",
    "# gc.collect()\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86eeb84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/accelerate/accelerator.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='222' max='292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [222/292 11:13:32 < 3:34:18, 0.01 it/s, Epoch 3.01/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 50\u001b[0m\n\u001b[1;32m     24\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     25\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[1;32m     26\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     warmup_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     45\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     46\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m     47\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_tok,\n\u001b[1;32m     48\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_tok,\n\u001b[1;32m     49\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/transformers/trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/accelerate/accelerator.py:2121\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2121\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tok,\n",
    "#     mlm=False,   # no random masking; we already set labels\n",
    "# )\n",
    "# data_collator = default_data_collator\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     evaluation_strategy=\"no\",\n",
    "#     eval_steps=500,\n",
    "#     logging_steps=100,\n",
    "#     save_steps=1000,\n",
    "#     save_total_limit=3,\n",
    "#     num_train_epochs=3,\n",
    "#     learning_rate=2e-4,\n",
    "#     warmup_ratio=0.03,\n",
    "#     fp16=torch.cuda.is_available(),\n",
    "#     bf16=False,\n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # to avoid warning\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a2c8c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(input_ids): 800\n",
      "len(labels):    800\n",
      "first 40 labels: tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n",
      "num supervised tokens: 0\n"
     ]
    }
   ],
   "source": [
    "ex = train_tok[0]\n",
    "print(\"len(input_ids):\", len(ex[\"input_ids\"]))\n",
    "print(\"len(labels):   \", len(ex[\"labels\"]))\n",
    "print(\"first 40 labels:\", ex[\"labels\"][:40])\n",
    "\n",
    "# Make sure there are some tokens != -100:\n",
    "print(\"num supervised tokens:\", sum(1 for t in ex[\"labels\"] if t != -100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)\n",
    "tok.save_pretrained(output_dir)\n",
    "print(\"Finished training + saved model + tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56de246",
   "metadata": {},
   "source": [
    "generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f8d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=output_dir,\n",
    "    tokenizer=tok,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "def generate_barbara_reply(query: str, max_new_tokens: int = 80):\n",
    "    input_text = (\n",
    "        \"You are Barbara. Answer in her natural WhatsApp style.\\n\\n\"\n",
    "        \"### QUERY\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"### INSTRUCTIONS\\n\"\n",
    "        \"Reply as Barbara would reply in WhatsApp.\"\n",
    "    )\n",
    "    prompt = input_text + \"\\n\\n### Barbara:\\n\"\n",
    "\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    if \"### Barbara:\" in out:\n",
    "        reply = out.split(\"### Barbara:\", 1)[1].strip()\n",
    "    else:\n",
    "        reply = out.strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "print(generate_barbara_reply(\"Hi, I'm sick today, can I get an extension?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d18c93",
   "metadata": {},
   "source": [
    "## Runing fine-tune (debuged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad35b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_text', 'target_text', 'label_source'],\n",
      "    num_rows: 1470\n",
      "})\n",
      "Train size: 1323 | Val size: 147\n",
      "Train human: 1024 | Train teacher: 299\n",
      "New train size after oversampling: 2347\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files={\"data\": OUTPUT_JSONL})[\"data\"]\n",
    "print(ds)\n",
    "\n",
    "splits = ds.train_test_split(test_size=0.1, seed=RANDOM_SEED)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"| Val size:\", len(val_ds))\n",
    "\n",
    "# splitting on human and teacher generated\n",
    "train_human   = train_ds.filter(lambda ex: ex.get(\"label_source\", \"\") == \"human\")\n",
    "train_teacher = train_ds.filter(lambda ex: ex.get(\"label_source\", \"\") == \"teacher\")\n",
    "print(\"Train human:\", len(train_human), \"| Train teacher:\", len(train_teacher))\n",
    "\n",
    "# oversampling human \n",
    "train_human_oversampled = concatenate_datasets([train_human] * HUMAN_DUP_FACTOR)\n",
    "train_balanced = concatenate_datasets([train_human_oversampled, train_teacher]).shuffle(seed=RANDOM_SEED)\n",
    "train_ds = train_balanced\n",
    "print(\"New train size after oversampling:\", len(train_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8d8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# 4-bit quantization (QLoRA style)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# model in 4-bit, note that automatically is must be placed on GPU\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "model = get_peft_model(base, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# to save up memory during training (we got a lot of out ofmemory errors)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644c19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c2870cfa2d479fbf89f5fe71e57c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2773c9a36b4f7d9746cc71f0f8879a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized train size: 2347 | val size: 147\n",
      "len(input_ids): 800\n",
      "len(labels):    800\n",
      "num supervised tokens: 736\n"
     ]
    }
   ],
   "source": [
    "def build_example(ex: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    input_text  = ex.get(\"input_text\", \"\")\n",
    "    target_text = ex.get(\"target_text\", \"\")\n",
    "\n",
    "    # propmt + full text\n",
    "    prompt = input_text + \"\\n\\n### Barbara:\\n\"\n",
    "    x = prompt + target_text\n",
    "\n",
    "    # full padded to max_len for batching\n",
    "    enc_full = tok(\n",
    "        x,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # prompt onl with NO padding \n",
    "    enc_prompt = tok(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    input_ids = enc_full[\"input_ids\"]\n",
    "    labels    = input_ids.copy()\n",
    "\n",
    "    # asking only pront\n",
    "    n_prompt = len(enc_prompt[\"input_ids\"])\n",
    "    n_prompt = min(n_prompt, len(labels))\n",
    "    for i in range(n_prompt):\n",
    "        labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": enc_full[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "train_tok = train_ds.map(\n",
    "    build_example,\n",
    "    remove_columns=train_ds.column_names,\n",
    ")\n",
    "val_tok = val_ds.map(\n",
    "    build_example,\n",
    "    remove_columns=val_ds.column_names,\n",
    ")\n",
    "\n",
    "train_tok.set_format(type=\"torch\", columns=cols)\n",
    "val_tok.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "print(\"Tokenized train size:\", len(train_tok), \"| val size:\", len(val_tok))\n",
    "ex0 = train_tok[0]\n",
    "print(\"len(input_ids):\", len(ex0[\"input_ids\"]))\n",
    "print(\"len(labels):   \", len(ex0[\"labels\"]))\n",
    "print(\"num supervised tokens:\", sum(1 for t in ex0[\"labels\"].tolist() if t != -100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c69c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/accelerate/accelerator.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219/219 12:19:33, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>0.097299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.101400</td>\n",
       "      <td>0.092053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.095500</td>\n",
       "      <td>0.091213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=219, training_loss=0.6032676092565876, metrics={'train_runtime': 44557.8061, 'train_samples_per_second': 0.158, 'train_steps_per_second': 0.005, 'total_flos': 3.48651690196992e+16, 'train_loss': 0.6032676092565876, 'epoch': 2.9846678023850086})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\", \n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=tok,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27daa46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training + saved model + tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/home/student/Whatsapp_webApp_-Django-/Fine_Tune/distilled/KB_lora\"\n",
    "trainer.save_model(output_dir)\n",
    "tok.save_pretrained(output_dir)\n",
    "print(\"Finished training + saved model + tokenizer.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
