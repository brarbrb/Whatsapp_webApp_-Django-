{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7a296d",
   "metadata": {},
   "source": [
    "In this part we applied a knowledge-distillation approach where a large model (Cohere Command + our RAG pipeline) acts as a teacher and a smaller TinyLlama model serves as the student.\n",
    "\n",
    "For selected messages, Cohere generated high-quality “teacher answers” based on the query, context, retrieved similar messages, and examples of some user's style. The student model was then fine-tuned on a mixed dataset that included both the real human replies and these teacher-generated replies.\n",
    "\n",
    "To preserve users authentic style, human-labeled examples were oversampled, while teacher examples were added selectively.\n",
    "For some important inputs, we created two versions of the same training sample—one with the human answer and one with the teacher answer—to enrich supervision.\n",
    "\n",
    "Through this setup, the student model learns user's tone while also benefiting from the teacher model clarity and completeness.\n",
    "\n",
    "Themain goal of this experimental part to see if the distillation technique can improve generalization and produces higher-quality from the smaller model. If so if we will have even bigger student model in theory we can dramatically reduce the resources in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed98d94",
   "metadata": {},
   "source": [
    "# Imports and env  settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, DataCollatorForLanguageModeling,Trainer,pipeline\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from typing import List, Dict, Optional, Any\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import cohere\n",
    "import json\n",
    "import os\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d41a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helpers & constants from the RAG file (generated automatically from ipynb)\n",
    "from RAG_generic_func import (\n",
    "    load_and_embedd_dataset,\n",
    "    create_pinecone_index,\n",
    "    upsert_vectors,       # we'll override here\n",
    "    build_context,\n",
    "    build_user_style,     # same\n",
    "    augment_prompt,\n",
    "    EMBEDDING_MODEL,\n",
    "    COHERE_API_KEY,\n",
    "    PINECONE_API_KEY,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a4ff0e",
   "metadata": {},
   "source": [
    "When running on VM we got errors from imports that were used in distillation part so we created separate imports. Errors were mostly from newer versiom packages like numpy not supporting the GPU we used (it was older version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ef118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules used during fine-tuning itself(on VM) \n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments,Trainer, default_data_collator,BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from typing import List, Dict, Optional, Any\n",
    "import torch\n",
    "import numpy as np\n",
    "import os \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ba9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COHERE_API_KEY = os.environ.get(\"COHERE_API_KEY_PAY\", \"\")\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\", \"\")\n",
    "\n",
    "output_dir = \"/home/student/Whatsapp_webApp_-Django-/Fine_Tune/distilled/KB_lora\"\n",
    "max_len = 800\n",
    "\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] # full attention\n",
    "# TARGET_MODULES = [\"q_proj\",\"v_proj\"]\n",
    "# TARGET_MODULES = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "KB_PATH = r\"RAG_data\\KB_data_distilled.csv\"\n",
    "OUTPUT_KB_JSONL = r\"RAG_data\\distillation_dataset.jsonl\"\n",
    "\n",
    "AUGMENT_FRACTION = 0.3   # fraction of human examples that also get a teacher-label version\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "USER = \"Barbara\" # the user whose tone we try to copy\n",
    "INDEX_NAME = \"chats-index\"\n",
    "\n",
    "# parameters for fine tuning\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "BATCH_SIZE = 4\n",
    "MAX_LENGTH = 512\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "# \"Weights\" via oversampling: how many times to duplicate human examples\n",
    "HUMAN_DUP_FACTOR = 2   # 2 = roughly double weight vs teacher\n",
    "OUTPUT_JSONL = \"RAG_data/distillation_dataset_clean.jsonl\"  # clean appended file\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d5d4db",
   "metadata": {},
   "source": [
    "# Distillation to improve fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb88098",
   "metadata": {},
   "source": [
    "## Getting teacher labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f7f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_style(\n",
    "    df: pd.DataFrame,\n",
    "    user_id: str,\n",
    "    k: int = 10,\n",
    "    text_col: str = \"text\",\n",
    "    random_sample: bool = True,\n",
    "    seed: int | None = 42,\n",
    ") -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      - list of example messages (lines)\n",
    "      - a single multi-line string user_style\n",
    "\n",
    "    If there are no messages for this user_id, returns ([], \"\").\n",
    "    \"\"\"\n",
    "    user_df = df[df[\"sender_user_id\"] == user_id].copy()\n",
    "\n",
    "    if len(user_df) == 0:\n",
    "        return [], \"\"\n",
    "\n",
    "    user_df = user_df.sort_values(\"sent_at\")\n",
    "\n",
    "    if random_sample and len(user_df) > k:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.choice(user_df.index.to_list(), size=k, replace=False)\n",
    "        user_df = user_df.loc[idx].sort_values(\"sent_at\")\n",
    "    else:\n",
    "        user_df = user_df.tail(k)\n",
    "\n",
    "    lines = [str(msg) for msg in user_df[text_col].tolist()]\n",
    "    user_style = \"\\n\".join(lines)\n",
    "    return lines, user_style\n",
    "\n",
    "\n",
    "def upsert_vectors(\n",
    "    index,               # Pinecone index object\n",
    "    dataset: pd.DataFrame,\n",
    "    embeddings: np.ndarray,\n",
    "    batch_size: int = 128,\n",
    "):\n",
    "    \"\"\"\n",
    "    Upsert vectors to a Pinecone index.\n",
    "\n",
    "    Args:\n",
    "        index: pc.Index instance.\n",
    "        dataset: DataFrame containing metadata; must align with embeddings.\n",
    "        embeddings: numpy array [n_rows, dim].\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    print(\"Upserting the embeddings to the Pinecone index...\")\n",
    "\n",
    "    if embeddings.shape[0] != len(dataset):\n",
    "        raise ValueError(\n",
    "            f\"Embeddings rows ({embeddings.shape[0]}) != dataset rows ({len(dataset)})\"\n",
    "        )\n",
    "\n",
    "    metadata_fields = [col for col in dataset.columns if col != \"embedding\"]\n",
    "\n",
    "    num_rows = embeddings.shape[0]\n",
    "    ids = [str(i) for i in range(num_rows)]\n",
    "\n",
    "    meta = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        entry = {col: row[col] for col in metadata_fields}\n",
    "        meta.append(entry)\n",
    "\n",
    "    to_upsert = list(zip(ids, embeddings, meta))\n",
    "\n",
    "    for i in tqdm(range(0, len(to_upsert), batch_size)):\n",
    "        i_end = min(i + batch_size, len(to_upsert))\n",
    "        index.upsert(vectors=to_upsert[i:i_end])\n",
    "\n",
    "    print(\"Upserting complete!\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ff9711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG initialization ===\n",
      "Loading and embedding the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a70a1ae9db4c2d9cd189055997af01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Rows for Barbara as receiver: 1181\n",
      "Creating a Pinecone index...\n",
      "Done!\n",
      "Upserting the embeddings to the Pinecone index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:36<00:00,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting complete!\n",
      "RAG initialization done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== RAG initialization ===\")\n",
    "\n",
    "# Cohere client\n",
    "co = cohere.Client(api_key=COHERE_API_KEY)\n",
    "\n",
    "# 1) Load KB\n",
    "whatsapp_chats = pd.read_csv(KB_PATH)\n",
    "\n",
    "# 2) Embed entire KB once\n",
    "model_emb = SentenceTransformer(EMBEDDING_MODEL)\n",
    "kb_df_all, embeddings = load_and_embedd_dataset(whatsapp_chats, model_emb)\n",
    "\n",
    "# 3) Keep only rows where Barbara is the receiver (for retrieval)\n",
    "kb_df_to_barbara = kb_df_all[kb_df_all[\"receiver_user_id\"] == USER].sort_values(\"conv_turn\")\n",
    "embeddings_to_barbara = embeddings[kb_df_to_barbara.index.to_list()]\n",
    "\n",
    "print(\"Rows for Barbara as receiver:\", len(kb_df_to_barbara))\n",
    "\n",
    "# 4) Create Pinecone index once\n",
    "pc = create_pinecone_index(INDEX_NAME, embeddings_to_barbara.shape[1])\n",
    "\n",
    "# 5) Upsert embeddings once\n",
    "index = pc.Index(INDEX_NAME)\n",
    "index = upsert_vectors(index, kb_df_to_barbara, embeddings_to_barbara)\n",
    "\n",
    "# 6) Shared context & style for Barbara (can tune conv_id)\n",
    "context = build_context(\n",
    "    kb_df_all,\n",
    "    conv_id=\"chat:u_1_u_2\",  # adjust conv_id as needed\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "_, user_style = build_user_style(\n",
    "    kb_df_all,\n",
    "    user_id=USER,\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "print(\"RAG initialization done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adeef431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohere_rag_answer(query: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Use cached embeddings + Pinecone index + user_style + context\n",
    "    to get a Cohere+RAG answer for a query.\n",
    "\n",
    "    Returns None if something fails.\n",
    "    \"\"\"\n",
    "    query = str(query).strip()\n",
    "    if not query:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        augmented_prompt, _ = augment_prompt(\n",
    "            query=query,\n",
    "            user_style=user_style,\n",
    "            context=context,\n",
    "            model=model_emb,\n",
    "            index=index,\n",
    "        )\n",
    "\n",
    "        response = co.chat(\n",
    "            model=\"command-a-03-2025\",\n",
    "            message=augmented_prompt,\n",
    "        )\n",
    "        text = response.text.strip()\n",
    "        if not text:\n",
    "            return None\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Cohere failed for query: {query[:60]!r}... ({e})\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_teacher_answer(query: str, kb_path: str = KB_PATH) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Wrapper used by the dataset builder.\n",
    "    Now uses the cached RAG state instead of re-embedding each time.\n",
    "    \"\"\"\n",
    "    return cohere_rag_answer(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b726ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_text(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Build the text that will go into the student model.\n",
    "    For now it's simple; later you can plug in full RAG context, etc.\n",
    "    \"\"\"\n",
    "    query = str(row[\"text\"]).strip()\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are {USER}. Answer in her natural WhatsApp style.\\n\\n\"\n",
    "        \"### QUERY\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"### INSTRUCTIONS\\n\"\n",
    "        f\"Reply as {USER} would reply in WhatsApp.\"\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e60e399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in KB: 2360\n",
      "Rows with receiver == 'Barbara' and non-empty human answer: 1181\n",
      "Base human examples: 1181\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(KB_PATH)\n",
    "\n",
    "df[\"answer\"] = df[\"answer\"].astype(str)\n",
    "mask_receiver_barbara = df[\"receiver_user_id\"] == USER\n",
    "\n",
    "# rows with non-empty human answer\n",
    "mask_has_human = df[\"answer\"].str.strip().ne(\"\")\n",
    "human_df = df[mask_receiver_barbara & mask_has_human].copy()\n",
    "print(f\"Total rows in KB: {len(df)}\")\n",
    "print(f\"Rows with receiver == {USER!r} and non-empty human answer: {len(human_df)}\")\n",
    "\n",
    "examples: List[Dict[str, Any]] = []\n",
    "for _, row in human_df.iterrows():\n",
    "    input_text = build_input_text(row)\n",
    "    human_answer = row[\"answer\"].strip()\n",
    "\n",
    "    examples.append({\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": human_answer,\n",
    "        \"label_source\": \"human\",   # used later for sampling/weighting\n",
    "    })\n",
    "\n",
    "print(f\"Base human examples: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3822fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will augment 354 rows with teacher answers\n",
      "Total examples after adding teacher labels: 1535\n"
     ]
    }
   ],
   "source": [
    "# randomly sampling rows where we add teacher\n",
    "indices = list(human_df.index)\n",
    "n_aug = int(AUGMENT_FRACTION * len(indices))\n",
    "augment_indices = set(random.sample(indices, n_aug))\n",
    "print(f\"Will augment {n_aug} rows with teacher answers\")\n",
    "\n",
    "for idx in augment_indices:\n",
    "    row = human_df.loc[idx]\n",
    "    query = str(row[\"text\"]).strip()\n",
    "    input_text = build_input_text(row)\n",
    "\n",
    "    teacher_answer = generate_teacher_answer(query)\n",
    "    if teacher_answer is None:\n",
    "        print(\"Haven't generated answer\")\n",
    "        continue\n",
    "\n",
    "    examples.append({\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": teacher_answer,\n",
    "        \"label_source\": \"teacher\",\n",
    "    })\n",
    "    \n",
    "print(f\"Total examples after adding teacher labels: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "040c3c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved distillation dataset to RAG_data\\distillation_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "out_df = pd.DataFrame(examples)\n",
    "out_df.to_json(\n",
    "    OUTPUT_KB_JSONL,\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")\n",
    "print(f\"Saved distillation dataset to {OUTPUT_KB_JSONL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1925f7",
   "metadata": {},
   "source": [
    "## Cleaningthe dataset - Run only when the KB wasn't clean enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query_from_input(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the text between '### QUERY' and '\\\\n\\\\n### INSTRUCTIONS'\n",
    "    from the input_text. Returns an empty string if pattern not found.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    marker_query = \"### QUERY\"\n",
    "    marker_instr = \"\\n\\n### INSTRUCTIONS\"\n",
    "    \n",
    "    pos_q = input_text.find(marker_query)\n",
    "    if pos_q == -1:\n",
    "        return \"\"\n",
    "    \n",
    "    # start after the line \"### QUERY\\n\"\n",
    "    pos_start = input_text.find(\"\\n\", pos_q)\n",
    "    if pos_start == -1:\n",
    "        return \"\"\n",
    "    pos_start += 1  # move past the newline\n",
    "    \n",
    "    pos_end = input_text.find(marker_instr, pos_start)\n",
    "    if pos_end == -1:\n",
    "        # take until the end if instructions marker not found\n",
    "        pos_end = len(input_text)\n",
    "    \n",
    "    query = input_text[pos_start:pos_end]\n",
    "    return query.strip()\n",
    "\n",
    "\n",
    "def cohere_barbara_reply(query: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Ask Cohere to answer as Barbara in WhatsApp style given just the query.\n",
    "    No RAG here - quick cleaning only.\n",
    "    \"\"\"\n",
    "    query = str(query).strip()\n",
    "    if not query:\n",
    "        return None\n",
    "    \n",
    "    prompt = (\n",
    "        \"You are Barbara. Answer in her natural WhatsApp style.\\n\\n\"\n",
    "        \"### QUERY\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"### INSTRUCTIONS\\n\"\n",
    "        \"Reply as Barbara would reply in WhatsApp. Use natural, short WhatsApp-style messages, \"\n",
    "        \"can include line breaks and emojis. Only output the reply, no explanations.\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        resp = co.chat(\n",
    "            model=\"command-r-08-2024\",\n",
    "            message=prompt,\n",
    "        )\n",
    "        text = resp.text.strip()\n",
    "        if not text:\n",
    "            return None\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Cohere failed for query: {query[:60]!r}... ({e})\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4196dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCRYPTION_LINE = (\n",
    "    \"Messages and calls are end-to-end encrypted. \"\n",
    "    \"Only people in this chat can read, listen to, or share them.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26d662f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(OUTPUT_KB_JSONL, lines=True)\n",
    "# Remove all mentions of the WhatsApp system message from input_text\n",
    "df[\"input_text\"] = df[\"input_text\"].str.replace(ENCRYPTION_LINE, \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d3957ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with audio-only query: 65\n",
      "Rows after dropping audio-only queries: 1470\n"
     ]
    }
   ],
   "source": [
    "def is_audio_only_query(input_text: str) -> bool:\n",
    "    q = extract_query_from_input(input_text)\n",
    "    # Clean possible invisible chars (like RTL mark) and lower\n",
    "    q_clean = q.replace(\"\\u200e\", \"\").strip().lower()\n",
    "    return (q_clean == \"audio omitted\") or (q_clean == \"\")\n",
    "\n",
    "\n",
    "audio_only_mask = df[\"input_text\"].apply(is_audio_only_query)\n",
    "print(\"Rows with audio-only query:\", audio_only_mask.sum())\n",
    "\n",
    "df = df[~audio_only_mask].copy()\n",
    "print(\"Rows after dropping audio-only queries:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cb0fb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with 'audio omitted' in target_text: 153\n"
     ]
    }
   ],
   "source": [
    "# Find rows where the *answer* contains \"audio omitted\"\n",
    "def has_audio_omitted_answer(target_text: str) -> bool:\n",
    "    if not isinstance(target_text, str):\n",
    "        return False\n",
    "    t_clean = target_text.replace(\"\\u200e\", \"\").lower()\n",
    "    return \"audio omitted\" in t_clean\n",
    "\n",
    "mask_audio_answer = df[\"target_text\"].apply(has_audio_omitted_answer)\n",
    "print(\"Rows with 'audio omitted' in target_text:\", mask_audio_answer.sum())\n",
    "\n",
    "# For each such row: generate a new Barbara-style answer using Cohere\n",
    "rows_to_fix = df[mask_audio_answer].copy()\n",
    "\n",
    "for idx, row in rows_to_fix.iterrows():\n",
    "    query = extract_query_from_input(row[\"input_text\"])\n",
    "    new_answer = cohere_barbara_reply(query)\n",
    "    \n",
    "    if new_answer is not None:\n",
    "        df.at[idx, \"target_text\"] = new_answer\n",
    "    else:\n",
    "        # If Cohere fails for some reason, you can either:\n",
    "        #  - keep the old target_text, or\n",
    "        #  - drop the row. Let's drop to keep dataset clean.\n",
    "        df = df.drop(index=idx)\n",
    "        print(f\"Dropped row {idx} because Cohere couldn't generate answer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b6f4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_JSONL = \"RAG_data/distillation_dataset_clean.jsonl\"  # clean appended file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38b6ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned rows: 1470\n",
      "Appended cleaned rows to RAG_data\\distillation_dataset_clean.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"Final cleaned rows:\", len(df))\n",
    "\n",
    "# Append to the existing file instead of overwriting\n",
    "with open(OUTPUT_JSONL, \"a\", encoding=\"utf-8\") as f:\n",
    "    for _, row in df.iterrows():\n",
    "        f.write(json.dumps(row.to_dict(), ensure_ascii=False))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Appended cleaned rows to {OUTPUT_JSONL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d18c93",
   "metadata": {},
   "source": [
    "## Runing fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad35b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_text', 'target_text', 'label_source'],\n",
      "    num_rows: 1470\n",
      "})\n",
      "Train size: 1323 | Val size: 147\n",
      "Train human: 1024 | Train teacher: 299\n",
      "New train size after oversampling: 2347\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files={\"data\": OUTPUT_JSONL})[\"data\"]\n",
    "print(ds)\n",
    "\n",
    "splits = ds.train_test_split(test_size=0.1, seed=RANDOM_SEED)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds   = splits[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"| Val size:\", len(val_ds))\n",
    "\n",
    "# splitting on human and teacher generated\n",
    "train_human   = train_ds.filter(lambda ex: ex.get(\"label_source\", \"\") == \"human\")\n",
    "train_teacher = train_ds.filter(lambda ex: ex.get(\"label_source\", \"\") == \"teacher\")\n",
    "print(\"Train human:\", len(train_human), \"| Train teacher:\", len(train_teacher))\n",
    "\n",
    "# oversampling human \n",
    "train_human_oversampled = concatenate_datasets([train_human] * HUMAN_DUP_FACTOR)\n",
    "train_balanced = concatenate_datasets([train_human_oversampled, train_teacher]).shuffle(seed=RANDOM_SEED)\n",
    "train_ds = train_balanced\n",
    "print(\"New train size after oversampling:\", len(train_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8d8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n"
     ]
    }
   ],
   "source": [
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# 4-bit quantization (QLoRA style)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# model in 4-bit, note that automatically is must be placed on GPU\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "model = get_peft_model(base, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# to save up memory during training (we got a lot of out ofmemory errors)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644c19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c2870cfa2d479fbf89f5fe71e57c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2773c9a36b4f7d9746cc71f0f8879a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized train size: 2347 | val size: 147\n",
      "len(input_ids): 800\n",
      "len(labels):    800\n",
      "num supervised tokens: 736\n"
     ]
    }
   ],
   "source": [
    "def build_example(ex: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    input_text  = ex.get(\"input_text\", \"\")\n",
    "    target_text = ex.get(\"target_text\", \"\")\n",
    "\n",
    "    # propmt + full text\n",
    "    prompt = input_text + \"\\n\\n### Barbara:\\n\"\n",
    "    x = prompt + target_text\n",
    "\n",
    "    # full padded to max_len for batching\n",
    "    enc_full = tok(\n",
    "        x,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # prompt onl with NO padding \n",
    "    enc_prompt = tok(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    input_ids = enc_full[\"input_ids\"]\n",
    "    labels    = input_ids.copy()\n",
    "\n",
    "    # asking only pront\n",
    "    n_prompt = len(enc_prompt[\"input_ids\"])\n",
    "    n_prompt = min(n_prompt, len(labels))\n",
    "    for i in range(n_prompt):\n",
    "        labels[i] = -100\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": enc_full[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "\n",
    "train_tok = train_ds.map(\n",
    "    build_example,\n",
    "    remove_columns=train_ds.column_names,\n",
    ")\n",
    "val_tok = val_ds.map(\n",
    "    build_example,\n",
    "    remove_columns=val_ds.column_names,\n",
    ")\n",
    "\n",
    "train_tok.set_format(type=\"torch\", columns=cols)\n",
    "val_tok.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "print(\"Tokenized train size:\", len(train_tok), \"| val size:\", len(val_tok))\n",
    "ex0 = train_tok[0]\n",
    "print(\"len(input_ids):\", len(ex0[\"input_ids\"]))\n",
    "print(\"len(labels):   \", len(ex0[\"labels\"]))\n",
    "print(\"num supervised tokens:\", sum(1 for t in ex0[\"labels\"].tolist() if t != -100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c69c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/accelerate/accelerator.py:479: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219/219 12:19:33, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>0.097299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.101400</td>\n",
       "      <td>0.092053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.095500</td>\n",
       "      <td>0.091213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=219, training_loss=0.6032676092565876, metrics={'train_runtime': 44557.8061, 'train_samples_per_second': 0.158, 'train_steps_per_second': 0.005, 'total_flos': 3.48651690196992e+16, 'train_loss': 0.6032676092565876, 'epoch': 2.9846678023850086})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\", \n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=tok,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27daa46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training + saved model + tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/home/student/Whatsapp_webApp_-Django-/Fine_Tune/distilled/KB_lora\"\n",
    "trainer.save_model(output_dir)\n",
    "tok.save_pretrained(output_dir)\n",
    "print(\"Finished training + saved model + tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44951ba3",
   "metadata": {},
   "source": [
    "Generation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a32f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=output_dir,\n",
    "    tokenizer=tok,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "def generate_barbara_reply(query: str, max_new_tokens: int = 80):\n",
    "    input_text = (\n",
    "        \"You are Barbara. Answer in her natural WhatsApp style.\\n\\n\"\n",
    "        \"### QUERY\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"### INSTRUCTIONS\\n\"\n",
    "        \"Reply as Barbara would reply in WhatsApp.\"\n",
    "    )\n",
    "    prompt = input_text + \"\\n\\n### Barbara:\\n\"\n",
    "\n",
    "    out = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    if \"### Barbara:\" in out:\n",
    "        reply = out.split(\"### Barbara:\", 1)[1].strip()\n",
    "    else:\n",
    "        reply = out.strip()\n",
    "\n",
    "    return reply\n",
    "\n",
    "print(generate_barbara_reply(\"Hi, I'm sick today, can I get an extension?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
