{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcaa6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch, numpy as np, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c6a7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi --query-gpu=gpu_name,compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b17eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39498720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bitsandbytes==0.43.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4510dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bitsandbytes as bnb\n",
    "# print(\"Bitsandbytes version:\", bnb.__version__)\n",
    "\n",
    "# from bitsandbytes.cuda_setup.main import get_compute_capabilities, get_cuda_lib_handle\n",
    "\n",
    "# try:\n",
    "#     print(\"CUDA handle:\", get_cuda_lib_handle())\n",
    "#     print(\"Compute capabilities:\", get_compute_capabilities())\n",
    "#     print(\"GPU kernels loaded OK ‚úÖ\")\n",
    "# except Exception as e:\n",
    "#     print(\"‚ö†Ô∏è CUDA load error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d4a2ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ep', 'scene', 'turn_idx', 'target_speaker', 'prompt', 'target'],\n",
       "        num_rows: 36701\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ep', 'scene', 'turn_idx', 'target_speaker', 'prompt', 'target'],\n",
       "        num_rows: 4738\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "data_files = {\n",
    "    \"train\": \"fine_tune_data/bbt_train.jsonl\",\n",
    "    \"validation\": \"fine_tune_data/bbt_val.jsonl\", \n",
    "    # \"test\": \"fine_tune_data/bbt_test.jsonl\"\n",
    "}\n",
    "ds = load_dataset(\"json\", data_files=data_files)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd8309ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
     ]
    }
   ],
   "source": [
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"   # light and friendly to 8GB\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "base = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "# base.gradient_checkpointing_enable()  # saves RAM - throws error because gpu not new enough\n",
    "\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM, r=8, lora_alpha=16, lora_dropout=0.05, target_modules=[\"q_proj\",\"v_proj\"])\n",
    "model = get_peft_model(base, config)\n",
    "model.enable_input_require_grads()\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00bd640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "\n",
    "def build_example(ex):\n",
    "    prompt = ex.get(\"prompt\",\"\")\n",
    "    target = ex.get(\"target\",\"\")\n",
    "    x = prompt + target\n",
    "    enc_full   = tok(x, max_length=max_len, truncation=True)\n",
    "    enc_prompt = tok(prompt, max_length=max_len, truncation=True)\n",
    "\n",
    "    input_ids = enc_full[\"input_ids\"]\n",
    "    labels    = input_ids.copy()\n",
    "\n",
    "    # mask prompt tokens\n",
    "    n_prompt = len(enc_prompt[\"input_ids\"])\n",
    "    labels[:n_prompt] = [-100]*min(n_prompt, len(labels))\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": enc_full[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "cols = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "train_tok = ds[\"train\"].map(build_example, remove_columns=ds[\"train\"].column_names)\n",
    "val_tok   = ds[\"validation\"].map(build_example, remove_columns=ds[\"validation\"].column_names)\n",
    "# test_tok = ds[\"test\"].map(build_example, remove_columns=ds[\"test\"].column_names)\n",
    "\n",
    "\n",
    "# set format to torch\n",
    "train_tok.set_format(type=\"torch\", columns=cols)\n",
    "val_tok.set_format(type=\"torch\", columns=cols)\n",
    "# test_tok.set_format(type=\"torch\", columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c196c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = TrainingArguments(\n",
    "#     output_dir=\"./bbt-lora\",\n",
    "#     num_train_epochs=2,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     gradient_accumulation_steps=16,      # effective batch ~16\n",
    "#     fp16=True,\n",
    "#     learning_rate=2e-4,\n",
    "#     logging_steps=50,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=200,\n",
    "#     save_steps=200,\n",
    "#     save_total_limit=2,\n",
    "#     report_to=\"none\",\n",
    "#     gradient_checkpointing=True,\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     warmup_ratio=0.03\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091c0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./bbt-lora\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # üëà avoids the warning\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c983a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4586' max='4586' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4586/4586 19:51:06, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.423200</td>\n",
       "      <td>2.359891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.359900</td>\n",
       "      <td>2.337797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.374300</td>\n",
       "      <td>2.331409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.384900</td>\n",
       "      <td>2.328696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.345300</td>\n",
       "      <td>2.322070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.307100</td>\n",
       "      <td>2.318727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.297700</td>\n",
       "      <td>2.312997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.340300</td>\n",
       "      <td>2.309741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.314500</td>\n",
       "      <td>2.306627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.271800</td>\n",
       "      <td>2.303357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.302100</td>\n",
       "      <td>2.300102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.210700</td>\n",
       "      <td>2.298369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.241300</td>\n",
       "      <td>2.298923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.257100</td>\n",
       "      <td>2.296855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.291800</td>\n",
       "      <td>2.294718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.287300</td>\n",
       "      <td>2.293268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.262500</td>\n",
       "      <td>2.293592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.275200</td>\n",
       "      <td>2.291716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.303500</td>\n",
       "      <td>2.291686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.256700</td>\n",
       "      <td>2.291179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.234200</td>\n",
       "      <td>2.291155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.221500</td>\n",
       "      <td>2.291038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4586, training_loss=2.3056248940602173, metrics={'train_runtime': 71480.9068, 'train_samples_per_second': 1.027, 'train_steps_per_second': 0.064, 'total_flos': 6.476606989686374e+16, 'train_loss': 2.3056248940602173, 'epoch': 1.9992915724367184})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # shift like in LM; logits is [bs, seq, vocab]\n",
    "    shift_logits = logits[:, :-1, :]\n",
    "    shift_labels = labels[:, 1:]\n",
    "    loss_mask = (shift_labels != -100)\n",
    "    # quick perplexity approx: cross-entropy on masked positions\n",
    "    import torch\n",
    "    shift_labels = torch.tensor(shift_labels)\n",
    "    loss_mask = torch.tensor(loss_mask)\n",
    "    shift_logits = torch.tensor(shift_logits)\n",
    "    ce = torch.nn.functional.cross_entropy(\n",
    "        shift_logits[loss_mask],\n",
    "        shift_labels[loss_mask],\n",
    "        reduction=\"mean\"\n",
    "    )\n",
    "    ppl = float(math.exp(ce.item())) if ce.isfinite() else float(\"inf\")\n",
    "    return {\"ppl\": ppl}\n",
    "\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a253daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Leonard>: Hey, you coming to lunch?\n",
      "<Howard>:No. I‚Äôm going to work. I‚Äôm doing this. The world will be better off. It‚Äôs the only way. I have a lot of work to do. I‚Äôm not going to talk to you. I‚Äôm sorry. I just wanted to make it clear. If you‚Äôre not going to talk to me, I‚Äôm not going to talk to\n"
     ]
    }
   ],
   "source": [
    "def generate_next(prompt, max_new_tokens=80):\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True, top_p=0.9, temperature=0.7,\n",
    "            eos_token_id=tok.eos_token_id\n",
    "        )\n",
    "    return tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "test_prompt = \"<Leonard>: Hey, you coming to lunch?\\n<Howard>:\"\n",
    "print(generate_next(test_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d2f1d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/Whatsapp_webApp_-Django-/peft/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bbt-lora/merged/tokenizer_config.json',\n",
       " './bbt-lora/merged/special_tokens_map.json',\n",
       " './bbt-lora/merged/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./bbt-lora/adapter\")\n",
    "tok.save_pretrained(\"./bbt-lora/adapter\")\n",
    "\n",
    "# Optional: merge LoRA into base weights (creates a full-size model)\n",
    "merged = model.merge_and_unload()\n",
    "merged.save_pretrained(\"./bbt-lora/merged\")\n",
    "tok.save_pretrained(\"./bbt-lora/merged\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
